\section{Numerical Experiments}

\label{sec:numerical}


Preliminary numerical results are presented in this section to provide additional insights into the performance guarantees of the gradient descent method \cref{eq:gd}.
We aim to elucidate that the final error attained by the gradient descent method \cref{eq:gd} is influenced by both the stepsize $\tau$ and the H{\"o}lder exponent $p$.

We generated the results using Julia \cite{Juliasirev} version 1.12 on an
Apple Macintosh Mini with a M2 processor, 8 performance cores,
and 32GB of memory. 

We have placed the Julia codes for the results in the GitHub repository
\url{https://github.com/ctkelley/Grad_Des_CKW.jl} with instructions for
reproducing the figures.

\subsection{Two-dimensional PDE with a non-Lipschitz term}

\label{subsec:example}

H{\"o}lder continuous gradients arise naturally in partial differential equations (PDEs) involving non-Lipschitz nonlinearity \cite{Barrett1991finite,Tang2025uniqueness}.
In this subsection,  we introduce a numerical example from \cite{Barrett1991finite}.
This problem is to solve the following two-dimensional PDE,
\begin{equation}
	\label{eq:cF}
	\cF (u) = - \Delta u + \nu u_+^{p} = 0,
\end{equation}
where $p \in (0,1)$, $\nu > 0$ is a constant and $u_+ = \max \{u, 0\}$.
It should be noted that 
$\cF$ is the gradient of the following energy functional,
\begin{equation*}
	\hat{f} (u) = \frac{1}{2} \|\nabla u\|^2 
+ \frac{\nu}{p+1} \int_D u_+^{p+1} (y) \, \rmd y.
\end{equation*}
%defined for $u \in H^1(D)$.


Discretizing \cref{eq:cF} with the standard five point difference scheme \cite{LeVeque2007finite} leads to the following nonlinear system,
\begin{equation}\label{F0}
	\bfF (\bfu) = \bfA \bfu + \nu \bfu_+^{1/2} - \bfb = 0,
\end{equation}
where $\bfA \in \Rnn$ is the discretization of $- \Delta$ with zero boundary conditions, $\bfb \in \Rn$ encodes the boundary conditions, and $\bfu_+^{1/2} = \max \{\bfu, 0\}^{1/2}$ is understood as a component-wise operation.
Problem~\cref{F0} is equivalent to optimization problem~\cref{opt:main} with
$\Omega=\mathbb{R}^n$, and
\begin{equation*}
	f (\bfu) =  \dfrac{1}{2}(f_1 (\bfu)+ f_2 (\bfu)) \quad
{\rm with} \quad  f_1 (\bfu)= \bfu\zz \bfA \bfu - 2\bfb\zz \bfu,
\quad  f_2 (\bfu)=\frac{\nu}{p+1} \bfe\zz \bfu_+^{1+p},
\end{equation*}
where $\bfe \in \Rn$ is the vector of all ones.

It is clear that $\nabla f_1$ is Lipschitz continuous with the Lipschitz constant  $L_1 = \norm{\bfA}$, and $\nabla f_2$ is locally H{\"o}lder continuous 
with $\alpha = 1/2$ and $L_2 = \nu n^{1/4}$ from
\begin{equation*}
	\norm{\nabla f_2 (\bfu) - \nabla f_2 (\bfv) }
	= \nu \norm{\bfu_+^{1/2} - \bfv_+^{1/2}}
	\leq \nu n^{1/4} \norm{\bfu - \bfv}^{1/2},
\end{equation*}
for all $\bfu, \bfv \in \Rn$. The function $f$ is $\lambda(\bfA)$-strongly convex, where
$\lambda(\bfA)$ is the smallest eigenvalue of the symmetric positive definite matrix $\bfA$.

We now modify the problem to enable direct computation of the errors in the 
iteration. To this end we follow Example 4.4 in \cite{QuBianChen} and
take as the exact solution the function
\[
%#u^*(x,y) = \left(\frac{3 r - 1}{2} \right)^{2p/(1-p)} \max(0, r-1/3)
u^*(x,y) = \left(\frac{3 r - 1}{2} \right)^{2} \max(0, r-1/3)
\]
where $r = \sqrt{x^2 + y^2}$, and let $\bfu^*$ be $u^*$ evaluated
at the interior grid points. We enforce the boundary conditions
\[
u(x,1) = u^*(x,1), u(x,0) = u^*(x,0), u(1,y) = u^*(1,y), u(0,y) = u^*(0,y)
\]
for $0 < x,y < 1$ and encode this into $\bfb$
Letting $\bfc^* = \bfF(\bfu*)$ out modified equation is 
\begin{equation}
\label{eq:problem1}
\bfF(\bfu) - \bfc^* = 0.
\end{equation}
Equation~\ref{eq:problem1} is the necessary condition for the optimization
problem
\begin{equation} \label{opt:test}
        \min_{\bfu \in \Rn} f (\bfu) = \dfrac{1}{2} \bfu\zz \bfA \bfu + \dfrac{1}{1 + p} \bfe\zz \bfu_{+}^{1 + p} - (\bfc^*)\zz \bfu.
\end{equation}


In the iteration we use the solution of $\bfA \bfu_0 = - \bfb$ as the 
initial iterate. This is the discretization of Laplace's equation
with the problem boundary conditions. In this way we ensure that the
entire iteration satisifies the boundary conditions. We use a $n \times n$
grid with $n=15$ for the examples in this section 

We then examine the effects of grid refinement in \S~\ref{subsubsec:alg1ex1}.

\subsection{Algorithm 1}
\label{subsubsec:alg1ex1}
In the first experiment, we scrutinize the performance of the gradient descent method \cref{eq:gd} under different stepsizes.
Specifically, with the parameters $p$ and $\nu$ fixed at $0.5$. 

We test the algorithm is tested for stepsizes of the form $\tau = \tau_0 h^2$,
where $h = 1/(n+1)$ is the spatial meshwidth and $\tau_0$ is taken from
the set $\{.2, .1, .05, .01\}$.

The corresponding numerical results, presented in \cref{subfig:stepsize}, illustrate the decay of the distance between the iterates and the global minimizer over iterations.
It can be observed that a larger stepsize facilitates a more rapid descent  in the early stage of iterations, albeit at the expense of a greater asymptotic error.
This phenomenon corroborates our theoretical predictions.


In the second experiment, we fix $\tau_0$ is fixed at $0.01$, 
while the parameter $p$ is varied over the values $\{0.2, 0.4, 0.6, 0.8\}$.
\Cref{subfig:alpha} similarly tracks the decay of the distance to the global minimizer over iterations.
It is evident that, as the value of $p$ decreases, the final error attained by the algorithm increases under the same stepsize.
Therefore, the associated optimization problems become increasingly ill-conditioned and thus more challenging to solve for smaller values of $p$.
These findings offer empirical support for our theoretical analysis.

\begin{figure}[h!]
	\centering
	\subfigure[different stepsizes]{
		\label{subfig:stepsize}
		\includegraphics[width=0.45\linewidth]{Figures/test_stepsize.pdf}
	}
	\subfigure[different values of $p$]{
		\label{subfig:alpha}
		\includegraphics[width=0.45\linewidth]{Figures/test_alpha.pdf}
	}
	\caption{Numerical performance of Algorithm 1 for problem~\cref{opt:test}.}
	\label{fig:gd}
\end{figure}

We now repeat the experiment with $n=31$, so we reduce the mesh width
by a factor of 2 and increase the norm of $\bfA$ by a factor of four.
As one would expect the stepsize must decrease by a factor of four
for stability.

\begin{figure}[h!]
        \centering
        \subfigure[different stepsizes]{
                \label{subfig:stepsize2}
                \includegraphics[width=0.45\linewidth]{Figures/test2_stepsize.pdf}
        }
        \subfigure[different values of $p$]{
                \label{subfig:alpha2}
                \includegraphics[width=0.45\linewidth]{Figures/test2_alpha.pdf}
        }
        \caption{Numerical performance of Algorithm 1 for problem~\cref{opt:test}.}
        \label{fig:gd2}
\end{figure} 


\subsection{Algorithm 2}
\label{subsubsec:alg2ex1}
