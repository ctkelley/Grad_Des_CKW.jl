\section{Numerical Experiments}

\label{sec:numerical}


Preliminary numerical results are presented in this section to provide additional insights into the performance guarantees of the gradient descent method \cref{eq:gd}.
We aim to elucidate that the final error attained by the gradient descent method \cref{eq:gd} is influenced by both the stepsize $\tau$ and the H{\"o}lder exponent $p$.

We generated the results using Julia \cite{Juliasirev} version 1.12 on an
Apple Macintosh Mini with a M2 processor, 8 performance cores,
and 32GB of memory. 

We have placed the Julia codes for the results in the GitHub repository
\url{https://github.com/ctkelley/Grad_Des_CKW.jl} with instructions for
reproducing the figures.

\subsection{Two-dimensional PDE with a non-Lipschitz term}

\label{subsec:example}

H{\"o}lder continuous gradients arise naturally in partial differential equations (PDEs) involving non-Lipschitz nonlinearity \cite{Barrett1991finite,Tang2025uniqueness}.
In this subsection,  we introduce a numerical example from \cite{Barrett1991finite}.
This problem is to solve the following two-dimensional PDE,
\begin{equation}
	\label{eq:cF}
	\cF (u) = - \Delta u + \nu u_+^{p} = 0,
\end{equation}
where $p \in (0,1)$, $\nu > 0$ is a constant and $u_+ = \max \{u, 0\}$.
It should be noted that 
$\cF$ is the gradient of the following energy functional,
\begin{equation*}
	\hat{f} (u) = \frac{1}{2} \|\nabla u\|^2 
+ \frac{\nu}{p+1} \int_D u_+^{p+1} (y) \, \rmd y.
\end{equation*}
%defined for $u \in H^1(D)$.


Discretizing \cref{eq:cF} with the standard five point difference scheme \cite{LeVeque2007finite} leads to the following nonlinear system,
\begin{equation}\label{F0}
	\bfF (\bfu) = \bfA \bfu + \nu \bfu_+^{1/2} - \bfb = 0,
\end{equation}
where $\bfA \in \Rnn$ is the discretization of $- \Delta$ with zero boundary conditions, $\bfb \in \Rn$ encodes the boundary conditions, and $\bfu_+^{1/2} = \max \{\bfu, 0\}^{1/2}$ is understood as a component-wise operation.
Problem~\cref{F0} is equivalent to optimization problem~\cref{opt:main} with
$\Omega=\mathbb{R}^n$, and
\begin{equation*}
	f (\bfu) =  \dfrac{1}{2}(f_1 (\bfu)+ f_2 (\bfu)) \quad
{\rm with} \quad  f_1 (\bfu)= \bfu\zz \bfA \bfu - 2\bfb\zz \bfu,
\quad  f_2 (\bfu)=\frac{\nu}{p+1} \bfe\zz \bfu_+^{1+p},
\end{equation*}
where $\bfe \in \Rn$ is the vector of all ones.

It is clear that $\nabla f_1$ is Lipschitz continuous with the Lipschitz constant  $L_1 = \norm{\bfA}$, and $\nabla f_2$ is locally H{\"o}lder continuous 
with $\alpha = 1/2$ and $L_2 = \nu n^{1/4}$ from
\begin{equation*}
	\norm{\nabla f_2 (\bfu) - \nabla f_2 (\bfv) }
	= \nu \norm{\bfu_+^{1/2} - \bfv_+^{1/2}}
	\leq \nu n^{1/4} \norm{\bfu - \bfv}^{1/2},
\end{equation*}
for all $\bfu, \bfv \in \Rn$. The function $f$ is $\lambda(\bfA)$-strongly convex, where
$\lambda(\bfA)$ is the smallest eigenvalue of the symmetric positive definite matrix $\bfA$.

We now modify the problem to enable direct computation of the errors in the 
iteration. To this end we follow Example 4.4 in \cite{QuBianChen} and
take as the exact solution the function
\[
%#u^*(x,y) = \left(\frac{3 r - 1}{2} \right)^{2p/(1-p)} \max(0, r-1/3)
u^*(x,y) = \left(\frac{3 r - 1}{2} \right)^{2} \max(0, r-1/3)
\]
where $r = \sqrt{x^2 + y^2}$, and let $\bfu^*$ be $u^*$ evaluated
at the interior grid points. We enforce the boundary conditions
\[
u(x,1) = u^*(x,1), u(x,0) = u^*(x,0), u(1,y) = u^*(1,y), u(0,y) = u^*(0,y)
\]
for $0 < x,y < 1$ and encode this into $\bfb$
Letting $\bfc^* = \bfF(\bfu*)$ out modified equation is 
\begin{equation}
\label{eq:problem1}
\bfF(\bfu) - \bfc^* = 0.
\end{equation}
Equation~\ref{eq:problem1} is the necessary condition for the optimization
problem
\begin{equation} \label{opt:test}
        \min_{\bfu \in \Rn} f (\bfu) = \dfrac{1}{2} \bfu\zz \bfA \bfu + \dfrac{1}{1 + p} \bfe\zz \bfu_{+}^{1 + p} - (\bfc^*)\zz \bfu.
\end{equation}


In the iteration we use the solution of $\bfA \bfu_0 = - \bfb$ as the 
initial iterate. This is the discretization of Laplace's equation
with the problem boundary conditions. In this way we ensure that the
entire iteration satisfies the boundary conditions. We use a $n \times n$
grid with $n=15$ for the examples in this section 

We then examine the effects of grid refinement in \S~\ref{subsubsec:alg1ex1}.

\subsection{Algorithm 1}
\label{subsubsec:alg1ex1}
In the first experiment, we scrutinize the performance of the gradient descent method \cref{eq:gd} under different stepsizes.
Specifically, with the parameters $p$ and $\nu$ fixed at $0.5$. 

We test the algorithm is tested for stepsizes of the form $\tau = \tau_0 h^2$,
where $h = 1/(n+1)$ is the spatial meshwidth and $\tau_0$ is taken from
the set $\{.2, .1, .05, .01\}$.

The corresponding numerical results, presented in \cref{subfig:stepsize}, illustrate the decay of the distance between the iterates and the global minimizer over iterations.
It can be observed that a larger stepsize facilitates a more rapid descent  in the early stage of iterations, albeit at the expense of a greater asymptotic error.
This phenomenon corroborates our theoretical predictions.


In the second experiment, we fix $\tau_0$ is fixed at $0.01$, 
while the parameter $p$ is varied over the values $\{0.2, 0.4, 0.6, 0.8\}$.
\Cref{subfig:alpha} similarly tracks the decay of the distance to the global minimizer over iterations.
It is evident that, as the value of $p$ decreases, the final error attained by the algorithm increases under the same stepsize.
Therefore, the associated optimization problems become increasingly ill-conditioned and thus more challenging to solve for smaller values of $p$.
These findings offer empirical support for our theoretical analysis.

\begin{figure}[h!]
	\centering
	\subfigure[different stepsizes]{
		\label{subfig:stepsize}
		\includegraphics[width=0.45\linewidth]{Figures/test_stepsize.pdf}
	}
	\subfigure[different values of $p$]{
		\label{subfig:alpha}
		\includegraphics[width=0.45\linewidth]{Figures/test_alpha.pdf}
	}
	\caption{Numerical performance of Algorithm 1 for problem~\cref{opt:test}.}
	\label{fig:gd}
\end{figure}

We now repeat the experiment with $n=31$, so we reduce the mesh width
by a factor of 2 and increase the norm of $\bfA$ by a factor of four.
As one would expect the stepsize must decrease by a factor of four
for stability.

\begin{figure}[h!]
        \centering
        \subfigure[different stepsizes]{
                \label{subfig:stepsize2}
                \includegraphics[width=0.45\linewidth]{Figures/test2_stepsize.pdf}
        }
        \subfigure[different values of $p$]{
                \label{subfig:alpha2}
                \includegraphics[width=0.45\linewidth]{Figures/test2_alpha.pdf}
        }
        \caption{Numerical performance of Algorithm 1 for problem~\cref{opt:test}.}
        \label{fig:gd2}
\end{figure} 


\subsection{Algorithm 2}
\label{subsubsec:alg2ex1}

We repeat the study for varying the exponent $p$ for Algorithm 2. We
set the parameter 
\[
\mu = 2 \pi^2
\]
which is the smallest eigenvalue of the Laplacian and 
a lower estimate for the actual value. We initialized the
step length to $.1 h^2$.
Comparing
Figure~\ref{fig:alpha3} to Figure~\ref{subfig:alpha2} shows the
benefits of the linesearch.
\begin{figure}[h!]
\label{fig:alpha3}
\includegraphics[width=5.in]{Figures/test3_alpha.pdf}
\caption{Numerical performance of Algorithm 2 for problem~\cref{opt:test}.}
\end{figure}

The advantage of the line search is that one does not manually adjust
the value of $\tau_0$ to converge for a given value of $\epsilon$.

\subsection{Algorithm 3}
\label{subsubsec:alg3ex1}

We report on two experiments. In both cases we set 
the algorithmic parameter $\nu = \tau_0 h^2$ ( this is not the same
as the parameter $\nu$ in example 1, which we always set to $1/2$).
As we did for Algorithms 1 and 2, we stop updating the solution if the
norm of the gradient increases.

{\bf We need to have a single parameter $\nu$ so we must change something.}

In the first example we use the values for $\tau_0$ from 
Figure~\ref{fig:gd}. In this way we can directly compare the performance
of Algorithm 3 with that of Algorithm 1. 

\begin{figure}[h!]
        \centering
        \subfigure[different stepsizes]{
                \label{subfig:stepsize3}
                \includegraphics[width=0.45\linewidth]{Figures/test_stepsize3.pdf}
        }
        \subfigure[different values of $p$]{
                \label{subfig:alpha3}
                \includegraphics[width=0.45\linewidth]{Figures/test_alpha3.pdf}
        }
        \caption{Numerical performance of Algorithm 3 for problem~\cref{opt:test}.}
        \label{fig:gd3}
\end{figure}

The results in Figure~\ref{fig:gd3} are poor. The reason for this is that
we are not exploiting the ability of Algorithm 3 to use larger stepsizes.
In Figure~\ref{fig:gd4} we consider larger values for $\tau_0$ in
Figure~\ref{subfig:stepsize4} and set $\tau_0 = 20$ in 
Figure~\ref{subfig:alpha4}.

{\bf Xiaojun, Liu, can we connect this to the estimate for fixed $\nu$
in Remark 4.2? I will be looking into this.}

\begin{figure}[h!] 
        \centering
        \subfigure[different stepsizes]{
                \label{subfig:stepsize4}
                \includegraphics[width=0.45\linewidth]{Figures/test_stepsize4.pdf}
        }
        \subfigure[different values of $p$]{ 
                \label{subfig:alpha4}
                \includegraphics[width=0.45\linewidth]{Figures/test_alpha4.pdf}
        } 
        \caption{Numerical performance of Algorithm 3 for problem~\cref{opt:test} with larger steps.}
        \label{fig:gd4}
\end{figure}

The convergence is much better in all cases. The hardest case ($p=.1$) 
has very irregular convergence in the terminal phae of the iteration.

\clearpage

\subsubsection{Step size and termination}
\label{subsubsec:term}

It is useful to look at the values of the step sizes from 
Remark 4.2. We note that for example 1, $M=O(h^{-2})$. 
We are using ${\hat \alpha} = p$ and neglecting constants in the 
estimate. So in Table~\ref{tab:nuvals} we tabulate
\begin{equation}
\label{eq:step24}
\nu = h^{p_1} \varepsilon^{p_2}
\end{equation}
where
\[
p_1 = (1 + p) / (1 + 3 p)
\mbox{}
p_2 = 2 (1 - p) / (1 + 3 p)
\]
for the case $h = 2^{-4}$.

\begin{table}[h!]
\caption{\label{tab:nuvals} Representative values of $\nu$}
\begin{center}
\begin{tabular}{lllll}
\hline 
$p \backslash 
\varepsilon$ & 1.00e-02 & 1.00e-03 & 1.00e-05 & 1.00e-08   \\      
1.00e-01 & 1.56e-05 & 6.43e-07 & 1.09e-09 & 7.68e-14   \\ 
2.00e-01 & 1.56e-04 & 1.56e-05 & 1.56e-07 & 1.56e-10   \\ 
5.00e-01 & 5.69e-03 & 2.26e-03 & 3.59e-04 & 2.26e-05   \\
8.00e-01 & 3.09e-02 & 2.36e-02 & 1.37e-02 & 6.08e-03   \\ 
\hline 
\end{tabular} 
\end{center}
\end{table}

Contrast the values of $\nu$ in the table to the value of 
$20 h^2 \approx .08$ and one can see that the step
size estimate from Equation~\ref{eq:step24} is very pessimistic. 
For smaller values of $p$ the predicted step is too small to be
useful in practice. 

{\bf Xiaojun, Lei, the bound should depend on M because of the 
mesh dependence of the convergence rate. Can you look into this?
I think the problem could be that in the proof you merge M into
the O-term. That does not reflect how the PDE problems work. Can
you put M in there somehow?
}

Next we consider the complexity bound 
\[
 O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \varepsilon^{-p_2})}.
\]
In Table~\ref{tab:itvals} we present the predicted number of iterations.
\begin{table}[h!]
\caption{\label{tab:itvals} Representative iteration number}
\begin{center}
\begin{tabular}{lllll}
\hline
$p \backslash 
\varepsilon$ & 1.00e-02 & 1.00e-03 & 1.00e-05 & 1.00e-08   \\ 
1.00e-01 & 3.91e+03 & 1.42e+05 & 1.39e+08 & 3.17e+12   \\ 
2.00e-01 & 6.64e+02 & 9.97e+03 & 1.66e+06 & 2.66e+09   \\ 
5.00e-01 & 4.19e+01 & 1.58e+02 & 1.66e+03 & 4.21e+04   \\ 
8.00e-01 & 1.14e+01 & 2.25e+01 & 6.44e+01 & 2.32e+02   \\ 
\hline 
\end{tabular} 
\end{center}
\end{table}

The estimates are pessimistic except for the larger values
of $p$ when compared to the findings we report in 
Figure~\ref{fig:gd4}.

Finally we consider termination of the iteration. In Example 1
we know the exact solution and can evaluate the algorithms in
terms of the error. In practice we cannot do that and must 
use the gradient norm as a surrogate for the error. While this is
standard for smooth optimization it could be a problem when the
gradient is not Lipschitz continuous. We illustrate this in
Figure~\ref{fig:resid}, where we compare the gradient norm with the
error for the case $p=.5$, $h=2^{-4}$, and $\tau_0 = 20 h^2$ using
Algorithm 3.

\begin{figure}[h!]
        \centering
        \subfigure[different stepsizes]{
                \label{subfig:res05}
                \includegraphics[width=0.45\linewidth]{Figures/restest05.pdf}
        }
        \subfigure[different values of $p$]{
                \label{subfig:res01}
                \includegraphics[width=0.45\linewidth]{Figures/restest01.pdf}
        }
        \caption{Gradient and error norms for problem~\cref{opt:test}.}
        \label{fig:resid}
\end{figure}

The results in the figure indicate that when the gradient norm stops
decreasing, the error has also stopped decreasing. However the
gradient norm is larger than the error norm, especially when
the error is small, which is consistent
with H{\"o}lder continuity.
