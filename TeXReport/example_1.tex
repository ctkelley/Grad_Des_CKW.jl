\section{Numerical Experiments}

\subsection{Two-dimensional PDE with a non-Lipschitz term}
\label{subsec:example}


H{\"o}lder continuous gradients arise naturally 
in partial differential equations (PDEs) involving non-Lipschitz nonlinearities
\cite{Barrett1991finite,Tang2025uniqueness}.
In this subsection,  we introduce a numerical example from \cite{Barrett1991finite}.
This problem is to solve the following two-dimensional PDE,
\begin{equation}
	\label{eq:cF}
	\cF (u) = - \Delta u + \nu u_+^{1/2} = c,
\end{equation}
where $\nu > 0$ is a constant and $u_+ = \max \{u, 0\}$.
It should be noted that $\cF$ is the gradient of the following energy functional,
\begin{equation*}
\hat{f} (u) = \frac{1}{2} \|\nabla u\|^2 + \frac{2 \nu}{3} \int_D u_+^{3/2} (y) \, \rmd y.
\end{equation*}
In \cref{eq:cF} we use a nonzero forcing term $c$ so that we can directly
compute the errors. We will further modify the forcing term after 
discretization to eliminate truncation error effects in our experiments.


Discretizing \cref{eq:cF} with the standard five point difference scheme \cite{LeVeque2007finite} leads to the following nonlinear system,
\begin{equation}\label{F0}
	\bfF (\bfu) = \bfA \bfu + \nu \bfu_+^{1/2} - \bfb = \bfc,
\end{equation}
where $\bfA \in \Rnn$ is the discretization of $- \Delta$ with zero boundary conditions, $\bfb \in \Rn$ encodes the boundary conditions, and $\bfu_+^{1/2} = \max \{\bfu, 0\}^{1/2}$ is understood as a component-wise operation.
Problem (\ref{F0}) is equivalent to  optimization problem (\ref{opt:main}) with
$\Omega=\mathbb{R}^n$, and
\begin{equation*}
	f (\bfu) =  \dfrac{1}{2}(f_1 (\bfu)+ f_2 (\bfu)) \quad
{\rm with} \quad  f_1 (\bfu)= \bfu\zz \bfA \bfu - 2\bfb\zz \bfu,
\quad  f_2 (\bfu)=\frac{4\nu}{3} \bfe\zz \bfu_+^{3/2},
\end{equation*}
where $\bfe \in \Rn$ is the vector of all ones.



It is clear that $\nabla f_1$ is Lipschitz continuous with the Lipschitz constant  $L_1 = \norm{\bfA}$, and $\nabla f_2$ is locally H{\"o}lder continuous with $\alpha = 1/2$ and $L_2 = \nu n^{1/4}$ from
\begin{equation*}
	\norm{\nabla f_2 (\bfu) - \nabla f_2 (\bfv) }
	= \nu \norm{\bfu_+^{1/2} - \bfv_+^{1/2}}
	\leq \nu n^{1/4} \norm{\bfu - \bfv}^{1/2},
\end{equation*}
for all $\bfu, \bfv \in \Rn$. The function $f$ is $\lambda(\bfA)$-strongly convex, where
$\lambda(\bfA)$ is the smallest eigenvalue of the symmetric positive definite matrix $\bfA$.

Other example from elliptic equations with a non-Lipschitz term is given in Section 5.


To evaluate the performance of the gradient descent method \cref{eq:gd}, we focus on the following optimization problem inspired by the PDE model introduced in \cref{subsec:example},
\begin{equation} \label{opt:test}
	\min_{\bfu \in \Rn} f (\bfu) = \dfrac{1}{2} \bfu\zz \bfA \bfu + \dfrac{1}{1 + \alpha} \bfe\zz \bfu_{+}^{1 + \alpha} - \bfc\zz \bfu,
\end{equation}
where $\bfA \in \Rnn$ is a symmetric positive definite matrix, $\alpha \in (0, 1)$ is a constant, and $\bfc = \bfA \bfu\uast + (\bfu\uast)_{+}^{\alpha} \in \Rn$ is a vector with $\bfu\uast \in \Rn$.
%and $\bfe \in \Rn$ is the vector of all ones.
%Let $\lambda_{\max}$ and $\lambda_{\min}$ represent the largest and smallest eigenvalue of $\bfA$, respectively.
%It is evident that the objective function $f$ is $\lambda_{\min}$-strongly convex and its gradient $\nabla f$ is locally $\alpha$-H{\"o}lder continuous with $\beta = 1 + \lambda_{\max}$ and $\gamma = 1$.
It is evident that the objective function $f$ is strongly convex with $\mu=\lambda_{\min}(\bfA)$ and its gradient $\nabla f$ is locally $\alpha$-H{\"o}lder continuous with $\beta = 1 + \lambda_{\max}(\bfA)$ and $\gamma = 1$.
Moreover, a straightforward verification reveals that $\bfu\uast$ is the minimizer of problem~\cref{opt:test}.


In our numerical experiments, the initial point $\bfu_{0}$, the minimizer $\bfu\uast$ and the matrix $\bfA$ in the test problem~\cref{opt:test} are generated randomly, and the vector $\bfc$ is defined by $\bfu\uast, \bfA$ and $\alpha$ with the detailed MATLAB code provided as follows.
\begin{lstlisting}
	u_0 = randn(n, 1);
	u_star = randn(n, 1);
	A = randn(n); A = A'*A + eye(n);
	c = A*u_star + max(u_star, 0).^alpha;
\end{lstlisting}
Moreover, the test problem dimension is fixed at $n = 50$, and method \cref{eq:gd} is permitted a maximum of $10000$ iterations.


In the first experiment, we scrutinize the performance of the gradient descent method \cref{eq:gd} under different stepsizes.
Specifically, with the parameter $\alpha$ fixed at $0.5$, the algorithm is tested for stepsizes chosen from the set $\{0.01, 0.005, 0.001, 0.0005\}$.
The corresponding numerical results, presented in \cref{subfig:stepsize}, illustrate the decay of the distance between the iterates and the global minimizer over iterations.
It can be observed that a larger stepsize facilitates a more rapid descent  in the early stage of iterations, albeit at the expense of a greater asymptotic error.
This phenomenon corroborates our theoretical predictions.


In the second experiment, the stepsize $\tau$ is fixed at $0.001$, while the parameter $\alpha$ is varied over the values $\{0.2, 0.4, 0.6, 0.8\}$.
\Cref{subfig:alpha} similarly tracks the decay of the distance to the global minimizer over iterations.
It is evident that, as the value of $\alpha$ decreases, the final error attained by the algorithm increases under the same stepsize.
Therefore, the associated optimization problems become increasingly ill-conditioned and thus more challenging to solve for smaller values of $\alpha$.
These findings offer empirical support for our theoretical analysis.



\begin{figure}[t]
	\centering
	\subfigure[different stepsizes]{
		\label{subfig:stepsize}
		\includegraphics[width=0.45\linewidth]{Figures/test_stepsize.pdf}
	}
	\subfigure[different values of $\alpha$]{
		\label{subfig:alpha}
		\includegraphics[width=0.45\linewidth]{Figures/test_alpha.pdf}
	}
	\caption{Numerical performance of gradient descent method \cref{eq:gd} for problem~\cref{opt:test}.}
	\label{fig:gd}
\end{figure}


