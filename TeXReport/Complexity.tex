% SIAM Article Template
\documentclass[review,hidelinks,onefignum,onetabnum]{siamart250211}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={HGD},
  pdfauthor={X. Chen, C. T. Kelley, and L. Wang}
}
\fi



\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
	This paper studies complexity of projected gradient descent methods for strongly convex constrained optimization problems where the objective function has $\alpha$-H{\"o}lder ($0 < \alpha \leq 1$) continuous gradient terms. We first show that with an appropriately fixed stepsize, the complexity bound for finding an approximate minimizer with a distance to the true minimizer less than $\varepsilon$ is $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\alpha - 1) / (1 + \alpha)})$, which extends the well-known complexity result for $\alpha = 1$. Next we show that the complexity bound can be improved to $O (\log (\varepsilon^{-1}) \varepsilon^{2( \alpha - 1) / (1+3\alpha)})$ if the stepsize is updated by the universal scheme. We illustrate our complexity results by numerical examples arising from elliptic equations with a non-Lipschitz term.
\end{abstract}

% REQUIRED
\begin{keywords}
Projected gradient descent, complexity, H{\"o}lder continuity
\end{keywords}

% REQUIRED
\begin{MSCcodes}
90C25, 65L05, 65Y20
\end{MSCcodes}





\section{Introduction}


Given a closed and convex set $\Omega \subseteq \Rn$, this paper considers the following optimization problem,
\begin{equation}
	\label{opt:main}
	\min_{\bfu \in \Omega} \hspace{2mm} f (\bfu) := \dfrac{1}{m} \sum_{i = 1}^{m} f_i (\bfu),
\end{equation}
where the objective function $f: \Rn \to \bR$  satisfies the following assumption.
\begin{assumption}
	\label{asp:function}
	\itshape
		
	\mbox{}
	
	\begin{enumerate}
		
		\item The function $f$ is $\mu$-strongly convex with a parameter $\mu > 0$ on $\Omega$, that is,
		\begin{equation*}
			%\label{eq:sc}
			f (\bfu) \geq f (\bfv) + \jkh{\nabla f (\bfv), \bfu - \bfv} + \dfrac{\mu}{2} \norm{\bfu - \bfv}^2,
		\end{equation*}
		for all $\bfu, \bfv \in \Omega$.
		
		\item For each $i \in [m]:= \{1, 2, \dotsc, m\}$, $f_i: \Rn \to \bR$ is continuously differentiable and the gradient $\nabla f_i$ is (globally) H{\"o}lder continuous with an exponent $\alpha_i \in (0,1]$ on $\Omega$, namely, there exists a constant $L_i > 0$ such that
		\begin{equation}
			\label{eq:Holder}
			\norm{\nabla f_i (\bfu) - \nabla f_i (\bfv)} \leq L_i \norm{\bfu - \bfv}^{\alpha_i},
		\end{equation}
		for all $\bfu, \bfv \in \Omega$.
	\end{enumerate}
	
\end{assumption}


Here, $\norm{\,\cdot\,}$ is the $\ell_2$ norm and $\jkh{\cdot, \cdot}$ is the inner product on $\Rn$.
We also denote by $\bfu\uast \in \Omega$ and $f\uast = f (\bfu\uast)$ the global minimizer and the optimal value of problem~\cref{opt:main}, respectively.
Let $\proj_{\Omega} (\cdot)$ be the projection operator onto the set $\Omega$ and $\hat{\alpha} = \min_{i \in [m]} \alpha_i \in (0, 1]$.

Suppose that each $\nabla f_i$ is Lipschitz continuous, which corresponds to condition~\cref{eq:Holder} with $\alpha_i = 1$ for all $\bfu, \bfv \in \Omega$.
Then $\nabla f$ is also Lipschitz continuous and the associated Lipschitz constant is $L = \sum_{i = 1}^{m} L_i / m$.
It is well known that the classical projected gradient descent method
\begin{equation}
	\label{eq:gd}
	\bfu_{k + 1} = \proj_{\Omega} \dkh{ \bfu_k - \tau \nabla f (\bfu_k) },
\end{equation}
with any initial point $\bfu_0 \in \Rn$ and the stepsize $\tau \in (0, 2 / (\mu + L)]$, achieves a linear rate of convergence \cite[Theorem  2.2.14]{Nesterov2018lectures} as follows,
\begin{equation*}
	\norm{\bfu_k - \bfu\uast} \leq \dkh{1 - \mu \tau}^k \norm{\bfu_0 - \bfu\uast}.
\end{equation*}
Therefore, for a given $\varepsilon > 0$, method (\ref{eq:gd}) is guaranteed to find a point $\bfu_k \in \Omega$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$ after at most $O (\log (\varepsilon^{-1}))$ iterations.
Unfortunately, this analysis fails if there exists at least one index $i \in [m]$ such that $\alpha_i < 1$. We explain the failure of the convergence of method (\ref{eq:gd}) to $\bfu\uast$ by the following example.

\begin{example}\cite[Example 1]{Chen2025new}
	Consider the following univariate optimization problem,
	\begin{equation*}
		\min_{x \in \bR} \hspace{2mm} f (x) = \dfrac{1}{2} x^2 + \dfrac{2}{3} \abs{x}^{3/2},
	\end{equation*}
	which is a special instance of problem~\cref{opt:main} with $f_1 (x) = x^2 / 2$, $f_2 (x) = 2 |x|^{3/2} / 3$, and $\Omega = \bR$.
	It is easy to see that the global minimizer is $x\uast = 0$.
	Method \cref{eq:gd} with the fixed stepsize $\tau > 0$ starting from $x_0 \neq 0$ reads as follows,
	\begin{equation*}
		x_{k + 1}
		= x_k - \tau \nabla f (x_k)
		= \dkh{1 - \tau} x_k - \tau \sign (x_k) \abs{x_k}^{1/2}.
	\end{equation*}
	A straightforward verification reveals that
	\begin{equation*}
		\abs{ x_{k + 1} }^2 - \abs{ x_k }^2
		= - \tau \dkh{2 - \tau} \abs{x_k}^2
		- 2 \tau \dkh{1 - \tau} \abs{ x_k }^{3/2}
		+ \tau^2 \abs{x_k}.
	\end{equation*}
	It is evident that, when $\abs{x_k}$ is sufficiently small, the last term in the right-hand side becomes dominant, resulting in that $| x_{k + 1} |^2 - | x_k |^2 \geq 0$.
		Therefore, the distance to the global minimizer ceases to decrease once it achieves a certain level.

Moreover, in \cite{Chen2025new} we show that $\nabla f$ is locally $\frac{1}{2}-$H\"older continuous, but not globally H\"older continuous. In fact, from
$$\nabla f(x\uast+|h|)-\nabla f(x\uast)=|h|+|h|^{\frac{1}{2}}
=(|h|^{1-\alpha}+|h|^{\frac{1}{2}-\alpha})|h|^{\alpha}=:(\hat{L}_1(h)+\hat{L}_2(h))|h|^{\alpha},
$$
we have $\hat{L}_1(h)\to \infty$ when $\alpha=(0,1)$ and
$|h|\to \infty$, while $\hat{L}_2(h)\to \infty$ when $\alpha=1$ and $|h|\to 0.$

This example demonstrates that a function $f$ expressed as a sum of component functions $f_i$, each endowed with a H{\"o}lder continuous gradient, may itself fail to possess a H{\"o}lder continuous gradient.
This phenomenon was  revisited by Nesterov (see \cite[Example 1]{Nesterov2025universal}).

On the other hand, this example satisfies Assumption 1.1 (ii) as
$$|\nabla f_1(x)-\nabla f_1(y)|\le L_1|x-y| \quad {\rm and} \quad |\nabla f_2(x)-\nabla f_2(y)|\le L_2|x-y|^{\frac{1}{2}}, \,\, \forall \, x,y \in \mathbb{R}$$
with $L_1=L_2=1.$
\end{example}

In \cite{Devolder2014first}, the authors presented the following bound for method (\ref{eq:gd})
\begin{equation*}
	f (\bfu_k) - f(\bfu\uast)\le K(N):= \frac{L_\alpha\|\bfu_0 - \bfu\uast\|^{1+\alpha}}{1+\alpha} \left( \frac{2}{N} \right)^\frac{1+\alpha}{2},
\end{equation*}
where $L_\alpha$ is the $\alpha-$H\"older Lipschitz constant  and  $\hat{\bfu}_N = \sum_{k=1}^N \bfu_k / N$.
In the  unconstrained case, (51) in \cite{Devolder2014first} comes to
\begin{equation*}
	\|\hat{\bfu}_N- \bfu\uast\|^2\le \frac{2}{\mu}K(N),
\end{equation*}
which implies that finding an $N$ average of iterations $\hat{\bfu}_N$ satisfying $\|\hat{\bfu}_N- \bfu\uast\| \le \epsilon$ requires $O(\epsilon^{-4/(1+\alpha)})$ iterations.


The contribution of this paper is to provide new complexity results of the projected gradient descent method
for problem (\ref{opt:main}) when the objective function is strongly convex, but its gradient is not Lipschitz due to
a $\alpha$-H{\"o}lder continuous term with $0 < \alpha < 1$. We first show that with an appropriately fixed stepsize, the complexity bound for finding the global minimizer less than $ \varepsilon$ is $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\alpha - 1) / (1 + \alpha)})$, which extends the well-known complexity result for $\alpha = 1$. Next we show that the complexity bound can be improved to $O (\log (\varepsilon^{-1}) \varepsilon^{2( \alpha - 1) / (1+3\alpha)})$ if the stepsize is updated at each step using the universal scheme. Our complexity bound is  at least $O(\epsilon^{-1})$ lower than (51) in \cite{Devolder2014first}. For example, when $\alpha = 1$, our bound is  $O(\log(\epsilon^{-1}))$ but (51) in \cite{Devolder2014first} is $O(\epsilon^{-2})$.

Our study is motivated by elliptic equations with a non-Lipschitz term \cite{Barrett1991finite,Tang2025uniqueness}, as well as optimization problems with a $\ell_p$-norm ($1 < p < 2$) regularization term  \cite{Baritaux2010efficient,Borges2018projection}.
We illustrate our complexity results by two numerical examples arising from elliptic equations with a non-Lipschitz term in Section 5, after we present complexity of projected gradient methods with fixed stepsize and updated stepsize in Sections 2-4, respectively.






%Now we point out that $\nabla f = \nabla f_1 + f_2$ fails to be H{\"o}lder continuous for any $\alpha \in (0, 1]$.
%In the special case of $\bfv = 0$, it is clear that
%\begin{equation*}
%	\nabla f (\bfu) - \nabla f (\bfv) = \bfA \bfu + \nu \bfu_+^{1/2}.
%\end{equation*}
%We investigate the following two cases.
%
%\noindent {\bf Case I: } $\alpha \in (0, 1)$.
%Since $\norm{\bfA \bfu} \geq \norm{\bfA^{-1}}^{-1} \norm{\bfu}$, we have
%\begin{equation*}
%	\norm{\nabla f (\bfu) - \nabla f (\bfv)}
%	\geq \norm{\bfA \bfu} - \nu \norm{\bfu_+^{1/2}}
%	\geq \norm{\bfA^{-1}}^{-1} \norm{\bfu} - \nu n^{1/4} \norm{\bfu}^{1/2}.
%\end{equation*}
%For any $L > 0$, we choose $\bfu \in \Rn$ satisfying
%\begin{equation*}
%	\norm{\bfu} \geq \max \hkh{ 4 \nu^2 n^{1/2} \norm{\bfA^{-1}}^2, \, \dkh{ 2 L \norm{\bfA^{-1}} }^{1/ (1 - \alpha)} }.
%\end{equation*}
%Then a straightforward verification reveals that
%\begin{equation*}
%	\norm{\nabla f (\bfu) - \nabla f (\bfv)}
%	\geq \dfrac{1}{2} \norm{\bfA^{-1}}^{-1} \norm{\bfu}
%	\geq L \norm{\bfu}^{\alpha}.
%\end{equation*}
%
%\noindent {\bf Case II: } $\alpha = 1$.
%For all $\bfu \in \Rn_+$, we have
%\begin{equation*}
%	\norm{\nabla f (\bfu) - \nabla f (\bfv)}
%	\geq \nu \norm{\bfu_+^{1/2}} - \norm{\bfA \bfu}
%	\geq \nu \norm{\bfu}^{1/2} - \norm{\bfA} \norm{\bfu}.
%\end{equation*}
%For any $L > 0$, we choose $\bfu \in \Rn_+$ satisfying
%\begin{equation*}
%	\norm{\bfu} \leq \dfrac{\nu^2}{(L + \norm{\bfA})^2}.
%\end{equation*}
%Then it can be readily verified that
%\begin{equation*}
%	\norm{\nabla f (\bfu) - \nabla f (\bfv)}
%	\geq L \norm{\bfu}.
%\end{equation*}
%Therefore, $\nabla f$ cannot be H{\"o}lder continuous for all $\alpha \in (0, 1]$.
%




%\subsection{Related Works}

%The minimization of convex functions with globally $\alpha$-H{\"o}lder continuous gradients is first studied by Devolder et al. \cite{Devolder2014first} in the form of inexact oracles.
%Following this work, Lan \cite{Lan2015bundle} develops an accelerated prox-level method for solving the class of composite problems.
%Moreover, Nesterov \cite{Nesterov2015universal} proposes universal gradient methods for minimizing composite functions which have globally $\alpha$-H{\"o}lder continuous gradients of its smooth part.
%However, to the best of our knowledge, the complexity bound of gradient descent method \cref{eq:gd} for a function $f$ satisfying the locally $\alpha$-H{\"o}lder continuous condition with $0 < \alpha < 1$ has not been developed.
%In general, extending existing results to the local setting typically requires the gradient to be uniformly bounded or to satisfy other comparable assumptions.
%By contrast, our subsequent analysis proceeds without invoking any such additional conditions.


%\subsection{Contribution}


%The contribution of this paper is to provide a new complexity result for the gradient descent method \cref{eq:gd} with a fixed stepsize $\tau$ to solve strongly convex optimization problems on $\Rn$ when the gradient is locally $\alpha$-H{\"o}lder continuous with $0 < \alpha < 1$, but not Lipschitz continuous. We show that method \cref{eq:gd} with an appropriately chosen $\tau$ can  find a point $\bfu_k$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$ after at most $O (\log (\varepsilon^{-1})\varepsilon^{2\alpha-2})$ iterations.
%Under the same problem setting, the complexity bound derived in \cite{Devolder2014first}, formulated in terms of the average of iterates rather than the current iterate, is $O (\varepsilon^{-4 / (1 + \alpha)})$, which is inferior to ours by at least a factor of $O (\varepsilon^{-1})$.





\section{Vanilla Projected Gradient Descent Method with a Fixed Stepsize}


In this section, we attempt to employ the vanilla projected gradient descent method \cref{eq:gd} with a fixed stepsize to solve problem~\cref{opt:main}, whose complexity bound is also provided.
Example 1.1 illustrates that the projected gradient descent method with a fixed stepsize \cref{eq:gd} will experience stagnation before reaching the global minimizer.



	
To obtain an approximate solution to problem~\cref{opt:main}, it is necessary to choose a sufficiently small stepsize $\tau$ in the projected gradient descent method \cref{eq:gd}, the magnitude of which depends on the desired level of accuracy.
Let $M > 0$ be a constant defined as
\begin{equation}
	\label{eq:const-m}
	M = \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i)} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
\end{equation}
We select a specific stepsize $\tau = \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} / M$ in the projected gradient descent method, whose complete framework is presented in \cref{alg:gd}.
Two sequences $\{\bfv_k\}$ and $\{\bfu_k\}$ are maintained in \cref{alg:gd}, where $\bfv_k$ is generated by the projected gradient descent method and $\bfu_k$ corresponds to the iterate achieving the smallest objective function value among the first $k$ iterations.


\begin{algorithm2e}[ht]
	%\SetAlgoLined
	\caption{Projected Gradient Descent Method (PGDM).}
	\label{alg:gd}
	
	\KwIn{$\varepsilon > 0$.}
	
	Initialize $\bfu_0 = \bfv_0 \in \Omega$.

	Choose the stepsize $\tau = \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} / M$.
		
	\For{$k = 0, 1, 2, \dotsc$}{
			
		Compute
		\begin{equation*}
			\bfv_{k + 1} = \proj_{\Omega} \dkh{ \bfv_k - \tau \nabla f (\bfv_k) }.
		\end{equation*}
		
		%Choose $\bar{k} = \max \{k\uast \mid k\uast \in \argmin_{l \in \{0, 1, \dotsc, k\}} f (\bfv_{l + 1})\}$.
		
		Set $\bfu_{k + 1} = \{\begin{array}{ll}
\bfv_{k+1}\,\, {\rm if } \, f( \bfv_{k + 1})\le f(\bfu_k)\\
\bfu_{k}   \,\, {\rm otherwise}
\end{array}$.
		
	}
	
	\KwOut{$\bfu_{k + 1}$.}
	
\end{algorithm2e}


Our subsequent analysis is based on the inexact oracle \cite{Devolder2014first} derived from the H{\"o}lder continuity condition of gradients, which is generalized to problem~\cref{opt:main} and demonstrated in the following proposition.


\begin{proposition}
	\label{prop:inexact}
	Suppose that \cref{asp:function} holds.
	Let $\delta > 0$ and
	\begin{equation*}
		\rho \geq \max_{i \in [m]} \hkh{ \fkh{ \dfrac{1 - \alpha_i}{(1 + \alpha_i) \delta} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
	\end{equation*}
	Then for all $\bfu, \bfv \in \Omega$, we have
	\begin{equation*}
		f (\bfv) \leq f (\bfu) + \jkh{\nabla f (\bfu), \bfv - \bfu} + \dfrac{\rho}{2} \norm{\bfv - \bfu}^2 + \dfrac{\delta}{2}.
	\end{equation*}
\end{proposition}


\begin{proof}
	Since $\nabla f_i$ is H{\"o}lder continuous with an exponent $\alpha_i$, we can obtain that
	\begin{equation*}
		f_i (\bfv)
		\leq f_i (\bfu)
		+ \jkh{\nabla f_i (\bfu), \bfv - \bfu}
		+ \dfrac{L_i}{1 + \alpha_i} \norm{\bfv - \bfu}^{1 + \alpha_i},
	\end{equation*}
	for all $\bfu, \bfv \in \Omega$.
	Then, for each $i$, it follows from \cite[Lemma 2]{Nesterov2015universal} that
	\begin{equation*}
		f_i (\bfv)
		\leq f_i (\bfu)
		+ \jkh{\nabla f_i (\bfu), \bfv - \bfu}
		+ \dfrac{\rho}{2} \norm{\bfv - \bfu}^2 + \dfrac{\delta}{2}.
	\end{equation*}
	Summing the above relationship over $i \in [m]$, we immediately arrive at the assertion of this proposition.
	The proof is completed.
\end{proof}


Now, we are in the position to derive the complexity bound of \cref{alg:gd} in the following theorem.


\begin{theorem}
	\label{thm:gd}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then after at most
	\begin{equation*}
		O \dkh{ \log \dkh{\dfrac{1}{\varepsilon}} \dfrac{1}{ \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} } }
	\end{equation*}
	iterations, \cref{alg:gd} will find an iterate $\bfu_k \in \Omega$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
\end{theorem}


\begin{proof}
	In view of \cref{prop:inexact}, we take
	\begin{equation*}
		\rho
		= \dfrac{1}{\tau}
		= \dfrac{M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}
		\geq \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
	\end{equation*}
	Then it holds that
	\begin{equation*}
		f (\bfv_{k + 1})
		\leq f (\bfv_k) + \jkh{\nabla f (\bfv_k), \bfv_{k + 1} - \bfv_k} + \dfrac{1}{2 \tau} \norm{\bfv_{k + 1} - \bfv_k}^2 + \dfrac{\mu \varepsilon^2}{4},
	\end{equation*}
	which, after a suitable rearrangement, can be equivalently written as
	\begin{equation}
		\label{eq:nabla-gd}
		\jkh{\nabla f (\bfv_k), \bfv_k - \bfv_{k + 1}}
		\leq f (\bfv_k) - f (\bfv_{k + 1})
		+ \dfrac{\mu \varepsilon^2}{4}
		+ \dfrac{1}{2 \tau} \norm{\bfv_{k + 1} - \bfv_k}^2.
	\end{equation}
	Recall that $f\uast = f (\bfu\uast)$.
	By virtue of the strong convexity of $f$, we can obtain that
	%	\begin{equation*}
		%		f\uast
		%		= f (\bfu\uast)
		%		\geq f (\bfu_k)
		%		+ \nabla f (\bfu_k)\zz (\bfu\uast - \bfu_k)
		%		+ \dfrac{\mu}{2} \norm{\bfu_k - \bfu\uast}^2,
		%	\end{equation*}
	\begin{equation}
		\label{eq:mu-grad}
		\jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_k}
		\leq f\uast - f (\bfv_k) - \dfrac{\mu}{2} \norm{\bfv_k - \bfu\uast}^2.
	\end{equation}
	The optimality condition of the projection problem defining $\bfv_{k + 1}$ yields that
	\begin{equation*}
		\jkh{\bfv_{k + 1} - \bfv_k + \tau \nabla f (\bfv_k), \bfu - \bfv_{k + 1}} \geq 0,
	\end{equation*}
	for all $\bfu \in \Omega$.
	Upon taking $\bfu = \bfu\uast$, we have
	\begin{equation*}
		\begin{aligned}
			\jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			\leq {} & \tau \jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_{k + 1}} \\
			= {} & \tau \jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_k}
			+ \tau \jkh{\nabla f (\bfv_k), \bfv_k - \bfv_{k + 1}},
		\end{aligned}
	\end{equation*}
	which together with \cref{eq:nabla-gd} and \cref{eq:mu-grad} implies that
	\begin{equation*}
		\begin{aligned}
			\jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			\leq {} & \tau \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} } - \dfrac{\mu \tau}{2} \norm{\bfv_k - \bfu\uast}^2 \\
			& + \dfrac{1}{2} \norm{\bfv_{k + 1} - \bfv_k}^2.
		\end{aligned}
	\end{equation*}
	Moreover, it can be readily verified that
	\begin{equation}
		\label{eq:vk-uast}
		\begin{aligned}
			\norm{\bfv_{k + 1} - \bfu\uast}^2
			= {} & \norm{\bfv_{k + 1} - \bfv_k + \bfv_k - \bfu\uast}^2 \\
			= {} & \norm{\bfv_k - \bfu\uast}^2
			+ 2 \jkh{\bfv_{k + 1} - \bfv_k, \bfv_k - \bfu\uast}
			+ \norm{\bfv_{k + 1} - \bfv_k}^2 \\
			= {} & \norm{\bfv_k - \bfu\uast}^2
			+ 2 \jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			- \norm{\bfv_{k + 1} - \bfv_k}^2.
		\end{aligned}
	\end{equation}
	Collecting the above two relationships together, we arrive at
	\begin{equation*}
		\norm{\bfv_{k + 1} - \bfu\uast}^2
		\leq \dkh{1 - \mu \tau} \norm{\bfv_k - \bfu\uast}^2
		+ 2 \tau \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} }.
	\end{equation*}
	From the construction of $\bfu_k$ in \cref{alg:gd}, it then follows that $f (\bfv_l) \geq f (\bfu_k)$ for all $l \in \{1, 2, \dotsc, k\}$.
	Let $C_k = \sum_{l = 1}^{k} \dkh{1 - \mu \tau}^{l - 1}$ be a constant.
	Applying the above relationship recursively for $k$ times leads to that
	\begin{equation*}
		\begin{aligned}
			\norm{\bfv_k - \bfu\uast}^2
			\leq {} & \dkh{ 1 - \mu \tau }^k \norm{\bfu_0 - \bfu\uast}^2
			+ 2 \tau \sum_{l = 1}^{k} \dkh{1 - \mu \tau}^{l - 1} \dkh{ f\uast - f (\bfv_l) + \dfrac{\mu \varepsilon^2}{4} } \\
			\leq {} & \dkh{ 1 - \mu \tau }^k \norm{\bfu_0 - \bfu\uast}^2
			+ 2 \tau \dkh{ f\uast - f (\bfu_k) + \dfrac{\mu \varepsilon^2}{4} } C_k,
		\end{aligned}
	\end{equation*}
	which together with $\norm{\bfv_k - \bfu\uast} \geq 0$ and $C_k \geq 1$ implies that
	\begin{equation*}
		f (\bfu_k) - f\uast
		\leq \dfrac{\dkh{ 1 - \mu \tau }^k}{2 \tau C_k} \norm{\bfu_0 - \bfu\uast}^2
		+ \dfrac{\mu \varepsilon^2}{4}
		\leq \dfrac{\dkh{ 1 - \mu \tau }^k}{2 \tau} \norm{\bfu_0 - \bfu\uast}^2
		+ \dfrac{\mu \varepsilon^2}{4}.
	\end{equation*}
	According to the strong convexity of $f$ and the optimality condition of problem~\cref{opt:main}, we have
	\begin{equation}
		\label{eq:dist}
		f (\bfu_k) - f\uast
		\geq \jkh{\nabla f (\bfu\uast), \bfu_k - \bfu\uast}
		+ \dfrac{\mu}{2} \norm{\bfu_k - \bfu\uast}^2
		\geq \dfrac{\mu}{2} \norm{\bfu_k - \bfu\uast}^2.
	\end{equation}
	Hence, it holds that
	\begin{equation*}
		\begin{aligned}
			\norm{\bfu_k - \bfu\uast}^2
			\leq {} & \dfrac{2}{\mu} \dkh{f (\bfu_k) - f\uast}
			\leq \dfrac{\dkh{ 1 - \mu \tau }^k}{\mu \tau} \norm{\bfu_0 - \bfu\uast}^2 + \dfrac{\varepsilon^2}{2} \\
			\leq {} & \dfrac{M \norm{\bfu_0 - \bfu\uast}^2}{\mu \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \dkh{ 1 - \dfrac{\mu}{M} \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} }^k + \dfrac{\varepsilon^2}{2}.
		\end{aligned}
	\end{equation*}
	We denote by $K\uast_\varepsilon$ the smallest iteration number $k$ such that $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
	Then solving the inequality $M \norm{\bfu_0 - \bfu\uast}^2 \varepsilon^{- 2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} ( 1 - \mu \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} / M )^k / \mu \leq \varepsilon^2 / 2$ indicates that
	\begin{equation*}
		\begin{aligned}
			K\uast_\varepsilon
			\leq {} & \dfrac{ 4 \log ( (2 M \norm{\bfu_0 - \bfu\uast}^2 / \mu)^{(1 + \hat{\alpha}) / 4} / \varepsilon ) }{ - \log ( 1 - \mu \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} / M ) (1 + \hat{\alpha}) } \\
			\leq {} & \dfrac{4 M \log ( (2 M \norm{\bfu_0 - \bfu\uast}^2 / \mu)^{(1 + \hat{\alpha}) / 4} / \varepsilon )}{\mu (1 + \hat{\alpha}) \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}.
		\end{aligned}
	\end{equation*}
	The proof is completed.
\end{proof}


\Cref{thm:gd} demonstrates that the iteration complexity of \cref{alg:gd} with a fixed stepsize is $O (\log (\varepsilon^{-1}) \varepsilon^{- 2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})})$ for problem~\cref{opt:main}.
This complexity result generalizes the classical linear convergence when $m = 1$ and $\hat{\alpha} = 1$, which highlights the performance degradation incurred by non-Lipschitz gradients.





\section{Universal Primal Gradient Method}


The fixed stepsize $\tau$ chosen in \cref{alg:gd} depends on the parameters $\alpha_i$ and $L_i$ for all $i \in [m]$, which are often unknown and hard to estimate in practice.
%certain problem-specific parameters of problem~\cref{opt:main}, including
To address this issue, we adopt the universal primal gradient method (UPGM) proposed by Nesterov \cite{Nesterov2015universal} to solve problem~\cref{opt:main}.
This method incorporates a line-search procedure to adaptively determine the stepsize at each iteration, and its overall framework is outlined in \cref{alg:upgm}.


\begin{algorithm2e}[ht]
	%\SetAlgoLined
	\caption{Universal Primal Gradient Method (UPGM).}
	\label{alg:upgm}
	
	\KwIn{$\varepsilon > 0$.}
	
	Initialize $\bfu_0 = \bfv_0 \in \Omega$ and $\rho_0 > 0$.
	
	\For{$k = 0, 1, 2, \dotsc$}{
		
		\For{$j_k = 0, 1, 2, \dotsc$}{
			
			Compute
			\begin{equation*}
				\bfv_{k + 1} = \proj_{\Omega} \dkh{ \bfv_k - \dfrac{1}{2^{j_k} \rho_k} \nabla f (\bfv_k) }.
			\end{equation*}
			
			{\bf If} $\bfv_{k + 1}$ satisfies the following line-search condition,
			\begin{equation}
				\label{eq:line-upgm}
				\begin{aligned}
					f (\bfv_{k + 1})
					\leq {} & f (\bfv_k)
					+ \jkh{\nabla f (\bfv_k), \bfv_{k + 1} - \bfv_k} \\
					& + \dfrac{2^{j_k} \rho_k}{2} \norm{\bfv_{k + 1} - \bfv_k}^2
					+ \dfrac{\mu \varepsilon^2}{4},
				\end{aligned}
			\end{equation}
			
			{\bf then}	break.
			
		}
		
		Update $\rho_{k + 1} = 2^{j_k} \rho_k$.
		
		Choose $\bar{k} = \max \{k\uast \mid k\uast \in \argmin_{l \in \{0, 1, \dotsc, k\}} f (\bfv_{l + 1})\}$.
		
		Set $\bfu_{k + 1} = \bfv_{\bar{k} + 1}$.
		
	}
	
	\KwOut{$\bfu_{k + 1}$.}
	
\end{algorithm2e}


Next, we establish the iteration complexity of \cref{alg:upgm}, which remains on the same order as that of the projected gradient descent method with a fixed stepsize.


\begin{theorem}
	\label{thm:upgm}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then after at most
	\begin{equation*}
		O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \dfrac{1}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} }
	\end{equation*}
	iterations, \cref{alg:upgm} will attain an iterate $\bfu_k \in \Omega$ satisfying that $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
\end{theorem}


\begin{proof}
	Obviously, there exists $j_k \in \bN$ such that
	\begin{equation*}
		2^{j_k} \rho_k \geq \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
	\end{equation*}
	By invoking the results of \cref{prop:inexact}, we know that condition~\cref{eq:line-upgm} is satisfied.
	Hence, the line-search step in \cref{alg:upgm} can be terminated after a finite number of trials and the required number of trials $j_k$ satisfies
	\begin{equation}
		\label{eq:jk-upgm}
		2^{j_k} \rho_k
		\leq 2 \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }
		\leq \dfrac{2 M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}},
	\end{equation}
	where $M > 0$ is a constant defined in \cref{eq:const-m}.
	Moreover, the line-search condition \cref{eq:line-upgm} directly yields that
	\begin{equation}
		\label{eq:nabla-upgm}
		\jkh{ \nabla f (\bfv_k), \bfv_k - \bfv_{k + 1} }
		\leq f (\bfv_k) - f (\bfv_{k + 1})
		+ \dfrac{2^{j_k} \rho_k}{2} \norm{\bfv_{k + 1} - \bfv_k}^2
		+ \dfrac{\mu \varepsilon^2}{4}.
	\end{equation}
	According to the optimality condition of the projection problem defining $\bfv_{k + 1}$, we have
	\begin{equation*}
		\jkh{\bfv_{k + 1} - \bfv_k + \dfrac{1}{2^{j_k} \rho_k} \nabla f (\bfv_k), \bfu\uast - \bfv_{k + 1}} \geq 0,
	\end{equation*}
	which further implies that
	\begin{equation*}
		\begin{aligned}
			\jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			\leq {} & \dfrac{1}{2^{j_k} \rho_k} \jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_{k + 1}} \\
			\leq {} & \dfrac{1}{2^{j_k} \rho_k} \jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_k}
			+ \dfrac{1}{2^{j_k} \rho_k} \jkh{\nabla f (\bfv_k), \bfv_k - \bfv_{k + 1}}.
		\end{aligned}
	\end{equation*}
	Substituting \cref{eq:mu-grad} and \cref{eq:nabla-upgm} into the above relationship leads to that
	\begin{equation*}
		\begin{aligned}
			\jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			\leq {} & \dfrac{1}{2^{j_k} \rho_k} \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} } \\
			& + \dfrac{1}{2} \norm{\bfv_{k + 1} - \bfv_k}^2
			- \dfrac{\mu}{2^{j_k + 1} \rho_k} \norm{\bfv_k - \bfu\uast}^2,
		\end{aligned}
	\end{equation*}
	Thus, it follows from relationship~\cref{eq:vk-uast} that
	\begin{equation*}
		\begin{aligned}
			\norm{\bfv_{k + 1} - \bfu\uast}^2
			\leq {} & \dkh{1 - \dfrac{\mu}{2^{j_k} \rho_k}} \norm{\bfv_k - \bfu\uast}^2
			+ \dfrac{2}{2^{j_k} \rho_k} \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} } \\
			\leq {} & \dkh{1 - \dfrac{\mu \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}{2 M}} \norm{\bfv_k - \bfu\uast}^2
			+ \dfrac{2}{\rho_0} \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} },
		\end{aligned}
	\end{equation*}
	where the last inequality comes from \cref{eq:jk-upgm} and $2^{j_k} \rho_k \geq \rho_0$.
	The remaining part of the proof follows the same line of reasoning as that of \cref{thm:gd} and is therefore omitted here for the sake of brevity.
\end{proof}


We end this section by estimating the total number of line-search steps required by \cref{alg:upgm}.


\begin{corollary}
	\label{coro:upgm}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then \cref{alg:upgm} requires at most
	\begin{equation*}
		O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \dfrac{1}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} }
	\end{equation*}
	line-search steps for the generated sequence $\{\bfu_k\}$ to satisfy $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
\end{corollary}


\begin{proof}
	Let $N_k$ be the total number of line-search steps after $k$ iterations in \cref{alg:upgm}.
	From the update rule $\rho_{k + 1} = 2^{j_k} \rho_k$, we can obtain that $j_k = \log \rho_{k + 1} - \log \rho_k$.
	Then a straightforward verification reveals that
	\begin{equation}
		\label{eq:nk}
		N_k = \sum_{l = 0}^{k} (j_l + 1)
		= k + 1 + \log \rho_{k + 1} - \log \rho_0,
	\end{equation}
	which together with relationship~\cref{eq:jk-upgm} implies that
	\begin{equation*}
		\begin{aligned}
			N_k
			\leq{} & k + 1 + \log \dkh{ \dfrac{2 M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} } - \log \rho_0 \\
			\leq {} & k
			+ \dfrac{2 (1 - \hat{\alpha})}{1 + \hat{\alpha}} \log \dkh{\dfrac{1}{\varepsilon}}
			+ \log \dkh{ \dfrac{2 M}{\rho_0} }
			+ 1.
		\end{aligned}
	\end{equation*}
	By invoking the results of \cref{thm:upgm}, we conclude that \cref{alg:upgm} requires at most $O ( \log (\varepsilon^{-1}) \varepsilon^{- 2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} )$ line-search steps, which completes the proof.
\end{proof}


At each iteration of \cref{alg:upgm}, we evaluate both the function value and the gradient at $\bfv_k$.
In addition, an extra function evaluation at $\bfv_{k + 1, j_k}$ is involved during each line-search step.
Therefore, \cref{thm:upgm} and \cref{coro:upgm} together reveal that the total number of function and gradient evaluations required by \cref{alg:upgm} is $O ( \log (\varepsilon^{-1}) \varepsilon^{- 2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} )$.





\section{Universal Fast Gradient Method}


To obtain a sharper complexity bound, we devise in this section a universal fast gradient method (UFGM) tailored to problem~\cref{opt:main}.
The proposed scheme, summarized in \cref{alg:ufgm}, exhibits slight but essential differences from the algorithm introduced by Nesterov \cite{Nesterov2015universal} to exploit the strong convexity of the objective function.


\begin{algorithm2e}[ht]
	%\SetAlgoLined
	\caption{Universal Fast Gradient Method (UFGM).}
	\label{alg:ufgm}
	
	\KwIn{$\varepsilon > 0$.}
	
	Initialize $\bfu_0 = \bfw_0 \in \Omega$, $\rho_0 \geq \mu$, and $\sigma_0 = 1$.
	
	\For{$k = 0, 1, 2, \dotsc$}{
		
		\For{$j_k = 0, 1, 2, \dotsc$}{
			
			Set $\nu_k = \sqrt{ \mu / (2^{j_k} \rho_k) }$, $\theta_k = \nu_k \sigma_k$, and $\sigma_{k + 1} = \sigma_k + \theta_k$.
			
			Compute
			\begin{equation}
				\label{eq:vk}
				\bfv_k = \dfrac{\sigma_k}{\sigma_k + \theta_k} \bfu_k + \dfrac{\theta_k}{\sigma_k + \theta_k} \proj_{\Omega} (\bfw_k),
			\end{equation}
			and
			\begin{equation}
				\label{eq:zk}
				\bfz_k = \proj_{\Omega} \dkh{ \proj_{\Omega} (\bfw_k) - \dfrac{\theta_k}{\sigma_k \mu} \nabla f (\bfv_k) }.
			\end{equation}
			
			Set
			\begin{equation}
				\label{eq:uk+1}
				\bfu_{k + 1} = \dfrac{\sigma_k}{\sigma_k + \theta_k} \bfu_k + \dfrac{\theta_k}{\sigma_k + \theta_k} \bfz_k.
			\end{equation}
			
			{\bf If} $\bfu_{k + 1}$ satisfies the following line-search condition,
			%		\begin{equation}
				%			\label{eq:local}
				%			\norm{\bfu_{k + 1} - \bfv_k} \leq \gamma,
				%		\end{equation}
			%		and
			\begin{equation}
				\label{eq:line}
				\begin{aligned}
					f (\bfu_{k + 1})
					\leq {} & f (\bfv_k)
					+ \jkh{\nabla f (\bfv_k), \bfu_{k + 1} - \bfv_k} \\
					& + \dfrac{\sigma_k^2 \mu}{2 \theta_k^2} \norm{\bfu_{k + 1} - \bfv_k}^2
					+ \dfrac{\theta_k \mu \varepsilon^2}{4 \sigma_{k + 1}},
				\end{aligned}
			\end{equation}
			
			{\bf then}	break.
			
		}
		
		Set $\rho_{k + 1} = 2^{j_k} \rho_k$ and update $\bfw_{k + 1}$ by
		\begin{equation}
			\label{eq:wk+1}
			\bfw_{k + 1} = \dfrac{1}{\sigma_{k + 1}} \dkh{ \sigma_k \bfw_k + \theta_k \bfv_k - \dfrac{\theta_k}{\mu} \nabla f (\bfv_k) }.
		\end{equation}
		
	}
	
	\KwOut{$\bfu_{k + 1}$.}
	
\end{algorithm2e}

%\begin{equation*}
%	\delta = \dfrac{\mu \varepsilon^2}{2}
%\end{equation*}

The following lemma illustrates that the line-search process in \cref{eq:line} is well-defined, which is guaranteed to terminate in a finite number of trials.


\begin{lemma}
	There exists an integer $j_k \in \bN$ such that the line-search condition \cref{eq:line} is satisfied in \cref{alg:ufgm}.
\end{lemma}


\begin{proof}
	It follows from the definition of $\theta_k$ and $\nu_k \leq 1$ that
	\begin{equation*}
		\dfrac{\theta_k}{\sigma_{k + 1}}
		= \dfrac{\theta_k}{\sigma_k + \theta_k}
		= \dfrac{\nu_k}{1 + \nu_k}
		\geq \dfrac{\nu_k}{2},
	\end{equation*}
	and
	\begin{equation*}
		\dfrac{\sigma_k^2 \mu}{\theta_k^2}
		= \dfrac{\mu}{\nu_k^2}
		= 2^{j_k} \rho_k.
	\end{equation*}
	Recall that $\hat{\alpha} = \min_{i \in [m]} \alpha_i \in (0, 1]$.
	Then we have
	\begin{equation*}
		\begin{aligned}
			\dfrac{\sigma_k^2 \mu}{\theta_k^2} \fkh{ \dfrac{\theta_k}{\sigma_{k + 1}} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
			\geq {} & \dfrac{2^{j_k} \rho_k}{2^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \nu_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \\
			= {} &  \dfrac{2^{j_k} \rho_k}{2^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \fkh{ \dfrac{\mu}{2^{j_k} \rho_k} }^{(1 - \hat{\alpha}) / (2 (1 + \hat{\alpha}))} \\
			= {} &  \dfrac{\mu^{(1 - \hat{\alpha}) / (2 (1 + \hat{\alpha}))}}{2^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \fkh{ 2^{j_k} \rho_k }^{(1 + 3 \hat{\alpha}) / (2 (1 + \hat{\alpha}))},
		\end{aligned}
	\end{equation*}
	where the first equality comes from the definition of $\nu_k$.
	Now it is clear that
	\begin{equation*}
		\dfrac{\sigma_k^2 \mu}{\theta_k^2} \fkh{ \dfrac{\theta_k}{\sigma_{k + 1}} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \to \infty,
	\end{equation*}
	as $j_k \to \infty$.
	Thus, there exists $j_k \in \bN$ such that
	\begin{equation}
		\label{eq:stepsize}
		\dfrac{\sigma_k^2 \mu}{\theta_k^2} \fkh{ \dfrac{\theta_k}{\sigma_{k + 1}} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
		\geq \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} },
	\end{equation}
	which further implies that
	\begin{equation*}
		\begin{aligned}
			\dfrac{\sigma_k^2 \mu}{\theta_k^2}
			\geq {} & \fkh{ \dfrac{\sigma_{k + 1}}{\theta_k} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} } \\
			\geq {} & \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i) \sigma_{k + 1}}{\mu (1 + \alpha_i) \theta_k \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
		\end{aligned}
	\end{equation*}
	As a direct consequence of \cref{prop:inexact}, we can proceed to show that the line-search condition \cref{eq:line} is satisfied, which completes the proof.
\end{proof}





\begin{lemma}
	\label{le:phi}
	Let $\{\phi_k\}$ be a sequence of functions defined recursively by
	\begin{equation}
		\label{eq:phi}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			= {} & \phi_k (\bfu)
			- \theta_k f\uast
			+ \theta_k f (\bfv_k)
			+ \theta_k \jkh{\nabla f (\bfv_k), \bfu - \bfv_k} \\
			& + \dfrac{\theta_k \mu}{2} \norm{\bfu - \bfv_k}^2,
		\end{aligned}
	\end{equation}
	with $\phi_0 (\bfu) = c_0 + \sigma_0 \mu \norm{\bfu - \bfw_0}^2 / 2$ for $c_0 = f (\bfu_0) - f\uast - \mu \varepsilon^2 / 4$, $\sigma_0 = 1$, and $\bfw_0 \in \Omega$.
	Then, for all $k \in \bN$, the function $\phi_k$ preserves the following canonical form,
	\begin{equation}
		\label{eq:phi-can}
		\phi_k (\bfu) = c_k + \dfrac{\sigma_k \mu}{2} \norm{\bfu - \bfw_k}^2,
	\end{equation}
	where $\{c_k\}$ is a sequence of real numbers and $\{\bfw_k\}$ is defined recursively by \cref{eq:wk+1}.
\end{lemma}


\begin{proof}
	We first prove that $\nabla^2 \phi_k = \sigma_k \mu I$ for all $k \in \bN$ by induction.
	It is evident that $\nabla^2 \phi_0 = \sigma_0 \mu I$.
	Now we assume that $\nabla^2 \phi_k = \sigma_k \mu I$ for some $k$.
	Then relationships \cref{eq:phi} and $\sigma_{k + 1} = \sigma_k + \theta_k$ imply that
	\begin{equation*}
		\nabla^2 \phi_{k + 1}
		= \nabla^2 \phi_k + \theta_k \mu I
		= \sigma_k \mu I + \theta_k \mu I
		= \sigma_{k + 1} \mu I.
	\end{equation*}
	Thus, we know that $\nabla^2 \phi_k = \sigma_k \mu I$ for all $k \in \bN$, which, in turn, justifies the canonical form of $\phi_k$ in \cref{eq:phi-can}.
	
	Next, by combining two relationships \cref{eq:phi} and \cref{eq:phi-can} together, we can obtain that
	\begin{equation*}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			= {} & c_k
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfu - \bfw_k}^2
			- \theta_k f\uast
			+ \theta_k f (\bfv_k) \\
			& + \theta_k \jkh{\nabla f (\bfv_k), \bfu - \bfv_k}
			+ \dfrac{\theta_k \mu}{2} \norm{\bfu - \bfv_k}^2.
		\end{aligned}
	\end{equation*}
	Since $\bfw_{k + 1}$ is a global minimizer of $\phi_{k + 1}$ over $\Rn$, the first-order optimality condition yields that
	\begin{equation*}
		\begin{aligned}
			0 = \nabla \phi_{k + 1} (\bfw_{k + 1})
			= {} & \sigma_k \mu (\bfw_{k + 1} - \bfw_k)
			+ \theta_k \nabla f (\bfv_k)
			+ \theta_k \mu (\bfw_{k + 1} - \bfv_k) \\
			= {} & \sigma_{k + 1} \mu \bfw_{k + 1}
			- \sigma_k \mu \bfw_k
			- \theta_k \mu \bfv_k
			+ \theta_k \nabla f (\bfv_k),
		\end{aligned}
	\end{equation*}
	from which the closed-form expression of $\bfw_{k + 1}$ in \cref{eq:wk+1} can be derived.
	The proof is completed.
\end{proof}


\begin{lemma}
	Let $\{\phi_k\}$ be the sequence of functions defined in \cref{le:phi}.
	Then we have
	\begin{equation}
		\label{eq:estimating}
		\phi_k (\bfu) \leq \sigma_k ( f (\bfu) - f\uast ) + \phi_0 (\bfu),
	\end{equation}
	for all $\bfu \in \Omega$ and $k \in \bN$.
\end{lemma}


\begin{proof}
	We prove that $\{\phi_k\}$ and $\{\sigma_k\}$ satisfy relationship~\cref{eq:estimating} by induction.
	It is obvious that \cref{eq:estimating} holds for $k = 0$ since $f (\bfu) \geq f\uast$ for any $\bfu \in \Omega$.
	Now we assume that \cref{eq:estimating} holds for some $k \in \bN$.
	It follows from the strong convexity of $f$ that
	\begin{equation*}
		f (\bfu) \geq f (\bfv_k) + \jkh{\nabla f (\bfv_k), \bfu - \bfv_k} + \frac{\mu}{2} \norm{\bfu - \bfv_k}^2,
	\end{equation*}
	for all $\bfu \in \Omega$.
	Then substituting the above relationship into \cref{eq:phi} leads to that
	\begin{equation*}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			\leq {} & \phi_k (\bfu)
			- \theta_k f\uast
			+ \theta_k f (\bfu) \\
			\leq {} & \sigma_k ( f (\bfu) - f\uast ) + \phi_0 (\bfu)
			+ \theta_k (f (\bfu) - f\uast) \\
			= {} & \sigma_{k + 1} (f (\bfu) -f\uast) + \phi_0 (\bfu),
		\end{aligned}
	\end{equation*}
	which indicates that \cref{eq:estimating} also holds for $k + 1$.
	We complete the proof.
\end{proof}


\begin{lemma}
	Let $\{\bfu_k\}$ be the sequence generated by \cref{alg:ufgm}.
	Then it holds that
	\begin{equation}
		\label{eq:descent}
		\sigma_k \dkh{ f (\bfu_k) - f\uast - \dfrac{\mu \varepsilon^2}{4} } \leq \phi_k\uast := \min_{\bfu \in \Omega} \phi_k (\bfu),
	\end{equation}
	for all $k \in \bN$.
\end{lemma}



\begin{proof}
	We aim to prove the assertion of this lemma by induction.
	It is clear that \cref{eq:descent} holds for $k = 0$ since $\sigma_0 = 1$ and $\phi_0\uast = \phi_0 (\bfw_0) = f (\bfu_0) - f\uast - \mu \varepsilon^2 / 4$.
	Now we assume that \cref{eq:descent} holds for some $k \in \bN$ and investigate the situation for $k + 1$.
	
	From the canonical form \cref{eq:phi-can}, it follows that $\phi_k$ is a strongly convex function and $\proj_{\Omega} (\bfw_k) = \argmin_{\bfu \in \Omega} \phi_k (\bfu)$.
	By invoking the result of \cite[Corollary 2.2.1]{Nesterov2018lectures}, we have
	\begin{equation*}
		\begin{aligned}
			\phi_k (\bfu)
			\geq {} & \phi_k\uast + \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2 \\
			\geq {} & \sigma_k \dkh{ f (\bfu_k) - f\uast - \dfrac{\mu \varepsilon^2}{4} } + \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2,
		\end{aligned}
	\end{equation*}
	for all $\bfu \in \Omega$.
	Then relationship~\cref{eq:phi} yields that
	\begin{equation*}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			\geq {} & \sigma_k \dkh{ f (\bfu_k) - f\uast - \dfrac{\mu \varepsilon^2}{4} }
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2
			- \theta_k f\uast \\
			& + \theta_k f (\bfv_k)
			+ \theta_k \jkh{\nabla f (\bfv_k), \bfu - \bfv_k}
			+ \dfrac{\theta_k \mu}{2} \norm{\bfu - \bfv_k}^2 \\
			\geq {} & \sigma_{k + 1} \dkh{f (\bfv_k) - f\uast}
			- \dfrac{\sigma_k \mu \varepsilon^2}{4}
			+ \jkh{\nabla f (\bfv_k), \sigma_k\bfu_k - \sigma_{k + 1} \bfv_k} \\
			& + \theta_k \jkh{\nabla f (\bfv_k), \bfu}
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2 \\
			= {} & \sigma_{k + 1} \dkh{f (\bfv_k) - f\uast}
			- \dfrac{\sigma_k \mu \varepsilon^2}{4}
			+ \theta_k \jkh{\nabla f (\bfv_k), \bfu - \proj_{\Omega} (\bfw_k)} \\
			& + \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2,
		\end{aligned}
	\end{equation*}
	where the second inequality comes from the strong convexity of $f$ and $\sigma_{k + 1} = \sigma_k + \theta_k$, and the last equality holds due to the definition of $\bfv_k$ in \cref{eq:vk}.
	According to the definition of $\bfz_k$ in \cref{eq:zk}, we can obtain that
	\begin{equation*}
		\begin{aligned}
			& \theta_k \jkh{\nabla f (\bfv_k), \bfu - \proj_{\Omega} (\bfw_k)}
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2 \\
			= {} & \dfrac{\sigma_k \mu}{2} \norm{ \bfu - \dkh{ \proj_{\Omega} (\bfw_k) - \dfrac{\theta_k}{\sigma_k \mu} \nabla f (\bfv_k) } }^2
			- \dfrac{\theta_k^2}{2 \sigma_k \mu} \norm{\nabla f (\bfv_k)}^2 \\
			\geq {} & \dfrac{\sigma_k \mu}{2} \norm{ \bfz_k - \dkh{ \proj_{\Omega} (\bfw_k) - \dfrac{\theta_k}{\sigma_k \mu} \nabla f (\bfv_k) } }^2
			- \dfrac{\theta_k^2}{2 \sigma_k \mu} \norm{\nabla f (\bfv_k)}^2 \\
			= {} & \theta_k \jkh{\nabla f (\bfv_k), \bfz_k - \proj_{\Omega} (\bfw_k)}
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfz_k - \proj_{\Omega} (\bfw_k)}^2
		\end{aligned}
	\end{equation*}
	As a result, it holds that
	\begin{equation}
		\label{eq:phi-u}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			\geq {} & \sigma_{k + 1} \dkh{ f (\bfv_k) - f\uast }
			- \dfrac{\sigma_k \mu \varepsilon^2}{4}
			+ \theta_k \jkh{\nabla f (\bfv_k), \bfz_k - \proj_{\Omega} (\bfw_k)} \\
			& + \dfrac{\sigma_k \mu}{2} \norm{\bfz_k - \proj_{\Omega} (\bfw_k)}^2,
		\end{aligned}
	\end{equation}
	for all $\bfu \in \Omega$.
	From the definitions of $\bfv_k$ and $\bfu_{k + 1}$ in \cref{eq:vk} and \cref{eq:uk+1}, it can be derived that $\theta_k (\bfz_k - \proj_{\Omega} (\bfw_k)) = \sigma_{k + 1} (\bfu_{k + 1} - \bfv_k)$.
	Substituting this relationship into \eqref{eq:phi-u} and taking $\bfu = \proj_{\Omega} (\bfw_{k + 1})$, we arrive at
	\begin{equation*}
		\dfrac{\phi_{k + 1}\uast}{\sigma_{k + 1}}
		\geq f (\bfv_k) - f\uast + \jkh{\nabla f (\bfv_k), \bfu_{k + 1} - \bfv_k}
		+ \dfrac{\sigma_k \sigma_{k + 1} \mu}{2 \theta_k^2} \norm{\bfu_{k + 1} - \bfv_k}^2
		- \dfrac{\sigma_k \mu \varepsilon^2}{4 \sigma_{k + 1}}.
	\end{equation*}
	which together with the line-search condition \cref{eq:line} and $\sigma_{k + 1} \geq \sigma_k$ implies that
	\begin{equation*}
		\begin{aligned}
			\dfrac{\phi_{k + 1}\uast}{\sigma_{k + 1}}
			\geq f (\bfu_{k + 1})
			- f\uast
			- \dfrac{\theta_k \mu \varepsilon^2}{4 \sigma_{k + 1}}
			- \dfrac{\sigma_k \mu \varepsilon^2}{4 \sigma_{k + 1}}
			= f (\bfu_{k + 1})
			- f\uast
			- \dfrac{\mu \varepsilon^2}{4}.
		\end{aligned}
	\end{equation*}
	Therefore, relationship \eqref{eq:descent} also holds for $k + 1$.
	The proof is completed.
\end{proof}


\begin{corollary}
	Let $\{\phi_k\}$ be the sequence of functions defined in \cref{le:phi} and $\{\bfu_k\}$ be the sequence generated by \cref{alg:ufgm}.
	Then we have
	\begin{equation}
		\label{eq:error}
		f (\bfu_k) - f\uast \leq \dfrac{1}{\sigma_k} \phi_0 (\bfu\uast) + \dfrac{\mu \varepsilon^2}{4},
	\end{equation}
	for any $k \in \bN$.
\end{corollary}


\begin{proof}
	Collecting two relationships \cref{eq:estimating} and \cref{eq:descent} together, we can obtain that
	\begin{equation*}
		\begin{aligned}
			\sigma_k \dkh{ f (\bfu_k) - f\uast - \dfrac{\mu \varepsilon^2}{4} }
			\leq {} & \min_{\bfu \in \Omega} \phi_k (\bfu)
			\leq \min_{\bfu \in \Omega} \hkh{ \sigma_k (f (\bfu) - f\uast) + \phi_0 (\bfu) } \\
			\leq {} & \sigma_k (f (\bfu\uast) - f\uast) + \phi_0 (\bfu\uast) \\
			= {} & \phi_0 (\bfu\uast),
		\end{aligned}
	\end{equation*}
	which completes the proof.
\end{proof}


We proceed to establish the iteration complexity of \cref{alg:ufgm}, as articulated in the theorem below.


\begin{theorem}
	\label{thm:ufgm}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then after at most
	\begin{equation*}
		O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \dfrac{1}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}} }
	\end{equation*}
	iterations, \cref{alg:ufgm} will reach an iterate $\bfu_k$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
\end{theorem}


\begin{proof}
	In view of relationship~\cref{eq:stepsize}, the number of line-search steps $j_k$ in \cref{eq:line} satisfies
	\begin{equation*}
		\begin{aligned}
			\dfrac{\sigma_k^2 \mu}{\theta_k^2} \fkh{ \dfrac{\theta_k}{\sigma_{k + 1}} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
			\leq {} & 2 \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} } \\
			\leq {} & \dfrac{2 M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}},
		\end{aligned}
	\end{equation*}
	where $M > 0$ is a constant defined in \cref{eq:const-m}.
	Since $\sigma_{k + 1} = \sigma_k + \theta_k = (1 + \nu_k) \sigma_k \leq 2 \sigma_k$, we arrive at
	\begin{equation}
		\label{eq:theta-sigma}
		\dfrac{\theta_k^2}{\sigma_k^2 \mu}
		\geq \dfrac{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}{2 M} \fkh{ \dfrac{\theta_k}{\sigma_{k + 1}} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
		\geq \dfrac{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}{2^{2 / (1 + \hat{\alpha})} M} \fkh{ \dfrac{\theta_k}{\sigma_k} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}.
	\end{equation}
	Let $\omega > 0$ be a constant defined as
	\begin{equation*}
		\omega = \dfrac{1}{2^{2 / (1 + 3 \hat{\alpha})}} \fkh{ \dfrac{\mu}{M} }^{(1 + \hat{\alpha}) / (1 + 3 \hat{\alpha})}.
	\end{equation*}
	Then it follows from relationship~\cref{eq:theta-sigma} that
	\begin{equation}
		\label{eq:theta}
		\theta_k
		\geq \sigma_k \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})},
	\end{equation}
	which further infers that
	\begin{equation*}
		\sigma_{k + 1}
		= \sigma_k + \theta_k
		\geq \sigma_k + \sigma_k \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}
		= \dkh{ 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} } \sigma_k.
	\end{equation*}
	Applying the above inequality for $k$ times recursively yields that
	\begin{equation*}
		\sigma_k \geq \dkh{ 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} }^k.
	\end{equation*}
	As a direct consequence of \cref{eq:dist} and \cref{eq:error}, we can show that
	\begin{equation*}
		\begin{aligned}
			\norm{\bfu_k - \bfu\uast}^2
			\leq {} & \dfrac{2}{\mu} \dkh{f (\bfu_k) - f\uast}
			\leq \dfrac{2}{\mu} \dkh{ \dfrac{1}{\sigma_k} \phi_0 (\bfu\uast) + \dfrac{\mu \varepsilon^2}{4} } \\
			\leq {} & \chi \dkh{ 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} }^{- k}
			+ \dfrac{\varepsilon^2}{2},
		\end{aligned}
	\end{equation*}
	where $\chi = 2 (f (\bfu_0) - f\uast) / \mu + \norm{\bfu_0 - \bfu\uast}^2 > 0$ is a constant.
	Let $K\uast_\varepsilon$ be the smallest iteration number $k$ such that $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
	By solving the inequality $\chi ( 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} )^{- k} \leq \varepsilon^2 / 2$, we have
	\begin{equation*}
		K\uast_\varepsilon
		\leq \log \dkh{ \dfrac{\sqrt{2 \chi}}{\varepsilon} } \dfrac{2}{\log \dkh{ 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} }}
		\leq \log \dkh{ \dfrac{\sqrt{2 \chi}}{\varepsilon} } \dfrac{4}{\omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}}.
	\end{equation*}
	The proof is completed.
\end{proof}


Building upon \cref{thm:ufgm}, we further demonstrate that the number of line-search steps required by \cref{alg:ufgm} is also $O ( \log (\varepsilon^{-1}) \varepsilon^{- 2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} )$.



\begin{corollary}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then, to achieve an iterate $\bfu_k$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$, \cref{alg:ufgm} requires at most
	\begin{equation*}
		O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \dfrac{1}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}} }
	\end{equation*}
	line-search steps.
\end{corollary}


\begin{proof}
	It follows from relationship~\cref{eq:theta-sigma} that
	\begin{equation*}
		\rho_{k + 1}
		= 2^{j_k} \rho_k
		= \dfrac{\sigma_k^2 \mu}{\theta_k^2}
		\leq \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \fkh{ \dfrac{\sigma_k}{\theta_k} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})},
	\end{equation*}
	which together with \cref{eq:theta} implies that
	\begin{equation*}
		\rho_{k + 1}
		\leq \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \fkh{ \dfrac{1}{\omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
		= \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\omega^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \varepsilon^{4 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}}.
	\end{equation*}
	Let $N_k$ be the total number of line-search steps after $k$ iterations in \cref{alg:ufgm}.
	In view of \cref{eq:nk}, we have
	\begin{equation*}
		\begin{aligned}
			N_k
			\leq{} & k + 1 + \log \dkh{ \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\omega^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \varepsilon^{4 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}} } - \log \rho_0 \\
			\leq {} & k
			+ \dfrac{4 (1 - \hat{\alpha})}{1 + 3 \hat{\alpha}} \log \dkh{\dfrac{1}{\varepsilon}}
			+ \log \dkh{ \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\omega^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \rho_0} }
			+ 1.
		\end{aligned}
	\end{equation*}
	Consequently, \cref{thm:ufgm} indicates that the total number of line-search steps in \cref{alg:ufgm} is at most $O ( \log (\varepsilon^{-1}) \varepsilon^{- 2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} )$, which completes the proof.
\end{proof}





\section{Numerical Experiments}


Preliminary numerical results are presented in this section to provide additional insights into the performance guarantees of the gradient descent method \cref{eq:gd}.
We aim to elucidate that the final error attained by the gradient descent method \cref{eq:gd} is influenced by both the stepsize $\tau$ and the H{\"o}lder exponent $\alpha$.
All codes are implemented in MATLAB R2018b on a workstation with dual Intel Xeon Gold 6242R CPU processors (at $3.10$ GHz$\times 20 \times 2$) and $510$ GB of RAM under Ubuntu 20.04.


\subsection{Two-dimensional PDE with a non-Lipschitz term}

\label{subsec:example}


H{\"o}lder continuous gradients arise naturally in partial differential equations (PDEs) involving non-Lipschitz nonlinearity \cite{Barrett1991finite,Tang2025uniqueness}.
In this subsection,  we introduce a numerical example from \cite{Barrett1991finite}.
This problem is to solve the following two-dimensional PDE,
\begin{equation}
	\label{eq:cF}
	\cF (u) = - \Delta u + \nu u_+^{1/2} = 0,
\end{equation}
where $\nu > 0$ is a constant and $u_+ = \max \{u, 0\}$.
It should be noted that $\cF$ is the gradient of the following energy functional,
\begin{equation*}
	\hat{f} (u) = \frac{1}{2} \|\nabla u\|^2 + \frac{2 \nu}{3} \int_D u_+^{3/2} (y) \, \rmd y.
\end{equation*}
%defined for $u \in H^1(D)$.


Discretizing \cref{eq:cF} with the standard five point difference scheme \cite{LeVeque2007finite} leads to the following nonlinear system,
\begin{equation}\label{F0}
	\bfF (\bfu) = \bfA \bfu + \nu \bfu_+^{1/2} - \bfb = 0,
\end{equation}
where $\bfA \in \Rnn$ is the discretization of $- \Delta$ with zero boundary conditions, $\bfb \in \Rn$ encodes the boundary conditions, and $\bfu_+^{1/2} = \max \{\bfu, 0\}^{1/2}$ is understood as a component-wise operation.
Problem (\ref{F0}) is equivalent to  optimization problem (\ref{opt:main}) with
$\Omega=\mathbb{R}^n$, and
\begin{equation*}
	f (\bfu) =  \dfrac{1}{2}(f_1 (\bfu)+ f_2 (\bfu)) \quad
{\rm with} \quad  f_1 (\bfu)= \bfu\zz \bfA \bfu - 2\bfb\zz \bfu,
\quad  f_2 (\bfu)=\frac{4\nu}{3} \bfe\zz \bfu_+^{3/2},
\end{equation*}
where $\bfe \in \Rn$ is the vector of all ones.



It is clear that $\nabla f_1$ is Lipschitz continuous with the Lipschitz constant  $L_1 = \norm{\bfA}$, and $\nabla f_2$ is locally H{\"o}lder continuous with $\alpha = 1/2$ and $L_2 = \nu n^{1/4}$ from
\begin{equation*}
	\norm{\nabla f_2 (\bfu) - \nabla f_2 (\bfv) }
	= \nu \norm{\bfu_+^{1/2} - \bfv_+^{1/2}}
	\leq \nu n^{1/4} \norm{\bfu - \bfv}^{1/2},
\end{equation*}
for all $\bfu, \bfv \in \Rn$. The function $f$ is $\lambda(\bfA)$-strongly convex, where
$\lambda(\bfA)$ is the smallest eigenvalue of the symmetric positive definite matrix $\bfA$.

Other example from elliptic equations with a non-Lipschitz term is given in Section 5.


%The strong convexity is equivalent to the following one-sided Lipschitz condition,
%\begin{equation*} %\label{eq:Lipschitz}
%	\dkh{\nabla f (\bfu) - \nabla f (\bfv)}\zz \dkh{\bfu - \bfv}
%	\geq \mu \norm{\bfu - \bfv}^2,
%\end{equation*}
%where $\mu$ is the parameter from \cref{eq:sc}.


%The energy function $f$ is strongly convex and therefore has a unique minimizer $\bfu\uast \in \Rn$.
%To see this, note that the monotonicity of $\bfu_+^{1/2}$ implies that
%\begin{equation*}
%	\dkh{\bfF (\bfu) - \bfF (\bfv)}\zz \dkh{\bfu - \bfv}
%	\geq \dkh{\bfA \dkh{\bfu - \bfv}}\zz \dkh{\bfu - \bfv}
%	\geq \lambda_{\min} \norm{\bfu - \bfv}^2,
%\end{equation*}
%for all $\bfu, \bfv \in \Rn$, where $\lambda_{\min}$ is the smallest eigenvalue of the positive definite matrix $\bfA$.


To evaluate the performance of the gradient descent method \cref{eq:gd}, we focus on the following optimization problem inspired by the PDE model introduced in \cref{subsec:example},
\begin{equation} \label{opt:test}
	\min_{\bfu \in \Rn} f (\bfu) = \dfrac{1}{2} \bfu\zz \bfA \bfu + \dfrac{1}{1 + \alpha} \bfe\zz \bfu_{+}^{1 + \alpha} - \bfc\zz \bfu,
\end{equation}
where $\bfA \in \Rnn$ is a symmetric positive definite matrix, $\alpha \in (0, 1)$ is a constant, and $\bfc = \bfA \bfu\uast + (\bfu\uast)_{+}^{\alpha} \in \Rn$ is a vector with $\bfu\uast \in \Rn$.
%and $\bfe \in \Rn$ is the vector of all ones.
%Let $\lambda_{\max}$ and $\lambda_{\min}$ represent the largest and smallest eigenvalue of $\bfA$, respectively.
%It is evident that the objective function $f$ is $\lambda_{\min}$-strongly convex and its gradient $\nabla f$ is locally $\alpha$-H{\"o}lder continuous with $\beta = 1 + \lambda_{\max}$ and $\gamma = 1$.
It is evident that the objective function $f$ is strongly convex with $\mu=\lambda_{\min}(\bfA)$ and its gradient $\nabla f$ is locally $\alpha$-H{\"o}lder continuous with $\beta = 1 + \lambda_{\max}(\bfA)$ and $\gamma = 1$.
Moreover, a straightforward verification reveals that $\bfu\uast$ is the minimizer of problem~\cref{opt:test}.


In our numerical experiments, the initial point $\bfu_{0}$, the minimizer $\bfu\uast$ and the matrix $\bfA$ in the test problem~\cref{opt:test} are generated randomly, and the vector $\bfc$ is defined by $\bfu\uast, \bfA$ and $\alpha$ with the detailed MATLAB code provided as follows.
\begin{lstlisting}
	u_0 = randn(n, 1);
	u_star = randn(n, 1);
	A = randn(n); A = A'*A + eye(n);
	c = A*u_star + max(u_star, 0).^alpha;
\end{lstlisting}
Moreover, the test problem dimension is fixed at $n = 50$, and method \cref{eq:gd} is permitted a maximum of $10000$ iterations.


In the first experiment, we scrutinize the performance of the gradient descent method \cref{eq:gd} under different stepsizes.
Specifically, with the parameter $\alpha$ fixed at $0.5$, the algorithm is tested for stepsizes chosen from the set $\{0.01, 0.005, 0.001, 0.0005\}$.
The corresponding numerical results, presented in \cref{subfig:stepsize}, illustrate the decay of the distance between the iterates and the global minimizer over iterations.
It can be observed that a larger stepsize facilitates a more rapid descent  in the early stage of iterations, albeit at the expense of a greater asymptotic error.
This phenomenon corroborates our theoretical predictions.


In the second experiment, the stepsize $\tau$ is fixed at $0.001$, while the parameter $\alpha$ is varied over the values $\{0.2, 0.4, 0.6, 0.8\}$.
\Cref{subfig:alpha} similarly tracks the decay of the distance to the global minimizer over iterations.
It is evident that, as the value of $\alpha$ decreases, the final error attained by the algorithm increases under the same stepsize.
Therefore, the associated optimization problems become increasingly ill-conditioned and thus more challenging to solve for smaller values of $\alpha$.
These findings offer empirical support for our theoretical analysis.



\begin{figure}[t]
	\centering
	\subfigure[different stepsizes]{
		\label{subfig:stepsize}
		\includegraphics[width=0.45\linewidth]{Figures/test_stepsize.pdf}
	}
	\subfigure[different values of $\alpha$]{
		\label{subfig:alpha}
		\includegraphics[width=0.45\linewidth]{Figures/test_alpha.pdf}
	}
	\caption{Numerical performance of gradient descent method \cref{eq:gd} for problem~\cref{opt:test}.}
	\label{fig:gd}
\end{figure}


{\bf Example 2}
We consider a numerical example motivated by a semi-linear elliptic problem with a constraint on the solution in a certain set \cite{Tang2025uniqueness}.
Let $D=(0,1)^3$ and
\begin{equation} \label{eq:cF2}
	\cH (u) = - \Delta u + \lambda |u|^{\nu} - |u|^{p-1} u
\end{equation}
on  $D$ with the boundary condition $u=1$ on the boundary $\partial D$, where $p > 1$, $\nu \in (0, 1)$ and $\lambda > p/\nu$ are  constants.
We consider the variational inequality that is to find $u^*\in [-1,1]$ such that
for any $u\in [-1,1]$,
\begin{equation*}
	\cH(u^*)(u-u^*)\ge 0.
\end{equation*}
This problem is equivalent to the nonlinear equation
\begin{equation}\label{exampleVI}
	0=\cF(u):=\left\{\begin{array}{ll}
		\cH(u)  & \quad {\rm if} \quad u-\cH(u) \in [-1, 1],\\
		u-1     & \quad {\rm if}  \quad u-\cH(u) \ge 1,\\
		u+1     & \quad {\rm otherwise.}
	\end{array}\right.
\end{equation}
Discretizing \cref{eq:cF2} with the standard five point difference scheme \cite{LeVeque2007finite}, problem~\cref{exampleVI} leads to the following  system of nonlinear equations
\begin{equation}\label{example2}
	\bfF(\bfu) = \bfu-{\mathrm \Pi}_{\bfU}\Big(\bfu- \tau(\bfA \bfu + \lambda |\bfu|^{\nu} - |\bfu|^{p -1}\bfu - \bfb)\Big) = 0,
\end{equation}
where $\bfU=[-1,1]^n$,  $\tau>0$ is a constant, $ \bfA\in \mathbb{R}^{n\times n}$ is a symmetric positive definite matrix and $\bfb\in \mathbb{R}^n.$  Note that \cref{example2} is the first-order optimal condition of the minimization problem
\begin{equation}\label{example2min}
	\min_{\bfu \in [-1,1]^n} f(\bfu):= \frac{1}{2}\bfu^\top\bfA\bfu + \frac{\lambda}{1+\nu} \bfe\zz |\bfu|^{\nu + 1}- \frac{1}{1+p}\bfe\zz  \max(\bfu, -\bfu)^{p+1} + \bfb^\top \bfu.
\end{equation}
The Hessian matrix of $f$ at $\bfu$ with $\bfu_i\neq 0$, $i=1,\ldots,n$ has the form
$$\nabla^2 f(\bfu)=\bfA  + \lambda \nu |\bfu|^{\nu-1} -p {\rm diag} \Big(\max (-\bfu, \bfu)^{p-1}\Big),$$
Since $\lambda\nu>p$, $\nabla^2 f(\bfu)$ is
symmetric positive definite for any $\bfu\in [-1, 1]^n$ with $\bfu_i\neq 0$, $i=1,\ldots,n$. Hence $f$ is $\mu$-strongly convex in $[-1,1]^n$ with $\mu=\lambda_{\min}(\bfA)$  and the system \cref{example2} has a unique solution in
$[-1, 1]^n.$ However, $\nabla f$ is not  Lipschitz continuous in $[-1,1]^n.$

Let
$$f_1(\bfu)=\frac{1}{2}\bfu^\top\bfA\bfu + \bfb^\top \bfu,  f_2(\bfu)=\frac{\lambda}{1+\nu} \bfe\zz |\bfu|^{\nu + 1}, f_3(\bfu)=- \frac{1}{1+p}\bfe\zz  \max(\bfu, -\bfu)^{p+1}$$
 This example satisfies Assumption 1.1 (ii) with $L_1=\lambda_{\max}(\bfA)$, $L_2=\lambda\nu$, $L_3=pn^{\frac{1}{2}},
 \alpha_1=\alpha_3=1, \alpha_2={1-\nu}.$  


\section{Conclusion}


In this paper, we have established a new complexity result for the projected gradient descent method with a fixed stepsize applied to strongly convex optimization problems where the objective function has a $\alpha$-H{\"o}lder continuous gradient term with $\alpha \in (0, 1]$.
The main conclusion is that, to achieve an approximate minimizer with a distance to the minimizer less than $\varepsilon$, the total number of iterations required by the gradient descent method \cref{eq:gd} is $O (\log (\varepsilon^{-1}) \varepsilon^{2 \alpha - 2})$ at the most.
This recovers the classical complexity result when $\alpha = 1$ and reveals the additional difficulty imposed by the weaker smoothness of the objective function for $\alpha < 1$.
Numerical experiments are conducted to validate our theoretical findings, demonstrating the expected behavior of gradient descent under different stepsizes and H{\"o}lder exponents.
These results offer new insights into the performance guarantees of the classic gradient descent method for a broader class of optimization problems with non-Lipschitz gradients.








%\section*{Acknowledgments}
%We would like to acknowledge the assistance of volunteers in putting
%together this example manuscript and supplement.



\bibliographystyle{siamplain}
\bibliography{library_HGD}


\end{document}
