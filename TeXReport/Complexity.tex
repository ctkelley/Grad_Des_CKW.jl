% SIAM Article Template
\documentclass[review,hidelinks,onefignum,onetabnum]{siamart250211}

% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{ex_shared}

% Optional PDF information
\ifpdf
\hypersetup{
  pdftitle={PGD},
  pdfauthor={X. Chen, C. T. Kelley, and L. Wang}
}
\fi



\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
	This paper studies the complexity of projected gradient descent methods for a class of strongly convex constrained optimization problems where the objective function is expressed as a summation of $m$ component functions, each possessing a gradient that is H{\"o}lder continuous with an exponent $\alpha_i \in (0, 1]$. Under this formulation, the gradient of the objective function may fail to be globally H{\"o}lder continuous, thereby existing complexity results inapplicable to this class of problems. Our theoretical analysis reveals that, in this setting, the complexity of projected gradient methods is determined by $\hat{\alpha} = \min_{i \in \{1, \cdots, m\}} \alpha_i$. We first prove that, with an appropriately fixed stepsize, the complexity bound for finding an approximate minimizer with a distance to the true minimizer less than $\varepsilon$ is $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + \hat{\alpha})})$, which extends the well-known complexity result for $\hat{\alpha} = 1$. Next we show that the complexity bound can be improved to $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + 3 \hat{\alpha})})$ if the stepsize is updated by the universal scheme. We illustrate our complexity results by numerical examples arising from elliptic equations with a non-Lipschitz term.
\end{abstract}

% REQUIRED
\begin{keywords}
projected gradient descent, complexity, H{\"o}lder continuity
\end{keywords}

% REQUIRED
\begin{MSCcodes}
90C25, 65L05, 65Y20
\end{MSCcodes}





\section{Introduction}


Given a closed and convex set $\Omega \subseteq \Rn$, this paper considers the following optimization problem,
\begin{equation}
	\label{opt:main}
	\min_{\bfu \in \Omega} \hspace{2mm} f (\bfu) := \dfrac{1}{m} \sum_{i = 1}^{m} f_i (\bfu),
\end{equation}
where the objective function $f: \Rn \to \bR$ satisfies the following assumption.
\begin{assumption}
	\label{asp:function}
	\mbox{}
	\begin{enumerate}
		
		\item The function $f$ is $\mu$-strongly convex with a parameter $\mu > 0$ on $\Omega$, that is,
		\begin{equation*}
			%\label{eq:sc}
			f (\bfu) \geq f (\bfv) + \jkh{\nabla f (\bfv), \bfu - \bfv} + \dfrac{\mu}{2} \norm{\bfu - \bfv}^2,
		\end{equation*}
		for all $\bfu, \bfv \in \Omega$.
		
		\item For each $i \in [m]:= \{1, 2, \dotsc, m\}$, the function $f_i: \Rn \to \bR$ is continuously differentiable and the gradient $\nabla f_i$ is (globally) H{\"o}lder continuous with an exponent $\alpha_i \in (0,1]$ on $\Omega$, namely, there exists a constant $L_i > 0$ such that
		\begin{equation}
			\label{eq:Holder}
			\norm{\nabla f_i (\bfu) - \nabla f_i (\bfv)} \leq L_i \norm{\bfu - \bfv}^{\alpha_i},
		\end{equation}
		for all $\bfu, \bfv \in \Omega$.
		
	\end{enumerate}
	
\end{assumption}


Here, $\norm{\,\cdot\,}$ is the $\ell_2$ norm and $\jkh{\cdot, \cdot}$ is the inner product on $\Rn$.
We also denote by $\bfu\uast \in \Omega$ and $f\uast = f (\bfu\uast)$ the global minimizer and the optimal value of problem~\cref{opt:main}, respectively.


Suppose that each $\nabla f_i$ is Lipschitz continuous, which corresponds to condition~\cref{eq:Holder} with $\alpha_i = 1$ for all $\bfu, \bfv \in \Omega$.
Then $\nabla f$ is also Lipschitz continuous and the associated Lipschitz constant is $L = \sum_{i = 1}^{m} L_i / m$.
Let $\proj_{\Omega} (\cdot)$ be the projection operator onto the set $\Omega$.
It is well known that the classical projected gradient descent method
\begin{equation}
	\label{eq:gd}
	\bfu_{k + 1} = \proj_{\Omega} \dkh{ \bfu_k - \tau \nabla f (\bfu_k) },
\end{equation}
with any initial point $\bfu_0 \in \Rn$ and the stepsize $\tau \in (0, 2 / (\mu + L)]$, achieves a linear rate of convergence \cite[Theorem  2.2.14]{Nesterov2018lectures} as follows,
\begin{equation*}
	\norm{\bfu_k - \bfu\uast} \leq \dkh{1 - \mu \tau}^k \norm{\bfu_0 - \bfu\uast}.
\end{equation*}
Therefore, for a given $\varepsilon > 0$, method~\cref{eq:gd} is guaranteed to find a point $\bfu_k \in \Omega$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$ after at most $O (\log (\varepsilon^{-1}))$ iterations.
Unfortunately, this analysis fails if there exists at least one index $i \in [m]$ such that $\alpha_i < 1$. We explain the failure of the convergence of method \cref{eq:gd} to $\bfu\uast$ by the following example.


\begin{example} \cite[Example 1]{Chen2025new}
	\label{exp:univ}
	Consider the following univariate optimization problem on $\Omega = \bR$,
	\begin{equation} \label{opt:univ}
		\min_{x \in \bR} \hspace{2mm} f (x) = \dfrac{1}{2} x^2 + \dfrac{2}{3} \abs{x}^{3/2},
	\end{equation}
	which is a special instance of problem~\cref{opt:main} with $f_1 (x) = x^2 / 2$ and $f_2 (x) = 2 |x|^{3/2} / 3$. 
	It is easy to see that the global minimizer is $x\uast = 0$.
	Method~\cref{eq:gd} with the fixed stepsize $\tau > 0$ starting from $x_0 \neq 0$ proceeds as follows,
	\begin{equation*}
		x_{k + 1}
		= x_k - \tau \nabla f (x_k)
		= \dkh{1 - \tau} x_k - \tau \sign (x_k) \abs{x_k}^{1/2},
	\end{equation*}
	where $\sign (x) = 1$ if $x > 0$, $0$ if $x = 0$, and $-1$ otherwise.
	A straightforward verification reveals that
	\begin{equation*}
		\abs{ x_{k + 1} }^2 - \abs{ x_k }^2
		= - \tau \dkh{2 - \tau} \abs{x_k}^2
		- 2 \tau \dkh{1 - \tau} \abs{ x_k }^{3/2}
		+ \tau^2 \abs{x_k}.
	\end{equation*}
	It is evident that, when $\abs{x_k}$ is sufficiently small, the last term in the right-hand side becomes dominant, resulting in that $| x_{k + 1} |^2 - | x_k |^2 \geq 0$.
	Therefore, the distance to the global minimizer ceases to decrease once it achieves a certain level.

	Moreover, in \cite{Chen2025new} we show that $\nabla f$ is locally, but not globally, H{\"o}lder continuous. 
	In fact, from
	\begin{equation*}
		\nabla f (\abs{h}) - \nabla f (0)
		= \abs{h} + \abs{h}^{1/2}
		= \dkh{ \abs{h}^{1 - \alpha} + \abs{h}^{1/2 - \alpha} } \abs{h}^{\alpha},
	\end{equation*}
	we can obtain that, $|h|^{1 - \alpha} \to \infty$ when $\alpha \in (0, 1)$ and $|h| \to \infty$, while $|h|^{1/2 - \alpha} \to \infty$ when $\alpha = 1$ and $|h| \to 0$. 
	Therefore, $\nabla f$ cannot be globally H{\"o}lder continuous for all $\alpha \in (0, 1]$. 
	
	On the other hand, problem~\cref{opt:univ} satisfies all the conditions in Assumption~\ref{asp:function}. 
	It is clear that $f$ is strongly convex. 
	In addition, we have
	\begin{equation*}
		\abs{\nabla f_1 (x) - \nabla f_1 (y)} = \abs{x - y},
	\end{equation*}
	and
	\begin{equation*}
		\abs{\nabla f_2 (x) - \nabla f_2 (y)}
		= \abs{\sign (x) \abs{x}^{1/2} - \sign (y) \abs{y}}
		\leq \sqrt{2} \abs{x - y}^{1/2},
	\end{equation*}
	for all $x, y \in \bR$. 
	
	This simple example demonstrates that, in problem~\cref{opt:main}, a function $f$ expressed as a sum of component functions $f_i$, each endowed with a H{\"o}lder continuous gradient, may itself fail to possess a H{\"o}lder continuous gradient.
	This phenomenon, initially observed in our previous work \cite{Chen2025new}, was later revisited and further highlighted by Nesterov (see \cite[Example 1]{Nesterov2025universal}).
\end{example}


Since $\nabla f$ may not be globally H{\"o}lder continuous, most existing complexity results are inapplicable to problem~\cref{opt:main}.
For the special case where $m = 1$, namely, $\nabla f$ is globally H{\"o}lder continuous with an exponent $\alpha \in (0,1]$, Devolder et al. \cite{Devolder2014first} presented the following bound for method~\cref{eq:gd},
\begin{equation*}
	f (\hat{\bfu}_N) - f(\bfu\uast)\le K(N):= \frac{L_\alpha\|\bfu_0 - \bfu\uast\|^{1+\alpha}}{1+\alpha} \left( \frac{2}{N} \right)^\frac{1+\alpha}{2},
\end{equation*}
where $L_\alpha$ is the H{\"o}lder constant  and $\hat{\bfu}_N = \sum_{k=1}^N \bfu_k / N$.
In the  strongly convex case, (51) in \cite{Devolder2014first} comes to
\begin{equation*}
	\norm{ \hat{\bfu}_N - \bfu\uast }^2 \leq \dfrac{2}{\mu} K(N),
\end{equation*}
which implies that finding an $N$ average of iterations $\hat{\bfu}_N$ satisfying $\|\hat{\bfu}_N- \bfu\uast\| \le \varepsilon$ requires $O(\varepsilon^{-4/(1 + \alpha)})$ iterations.


The contribution of this paper is to provide new complexity results of the projected gradient descent methods for problem~\cref{opt:main}, which are dictated by the parameter $\hat{\alpha} = \min_{i \in [m]} \alpha_i \in (0, 1]$.
We first show that, with an appropriately fixed stepsize, the complexity bound for finding an iterate with a distance to the global minimizer less than $\varepsilon$ is $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + \hat{\alpha})})$, which extends the well-known complexity result for $\hat{\alpha} = 1$.
Next, we demonstrate that this complexity bound can be improved to $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + 3\hat{\alpha})})$ if the stepsize is updated at each iteration using the universal scheme.
Even in the special case where $m = 1$, our complexity bound is at least $O(\varepsilon^{-1})$ lower than (51) in \cite{Devolder2014first}.
For example, when $\hat{\alpha} = 1 / 2$, our bound is  $O(\log(\varepsilon^{-1}) \varepsilon^{-2/5})$ but (51) in \cite{Devolder2014first} is $O(\varepsilon^{-8/3})$.


Our study is motivated by elliptic equations with a non-Lipschitz term \cite{Barrett1991finite,Tang2025uniqueness}, as well as optimization problems with an $\ell_p$-norm ($1 < p < 2$) regularization term  \cite{Baritaux2010efficient,Borges2018projection}.
We illustrate our complexity results by two numerical examples arising from elliptic equations with a non-Lipschitz term in \cref{sec:numerical}, after we present complexity of projected gradient methods with fixed stepsizes and updated stepsizes in \cref{sec:pgdm,sec:upgm,sec:ufgm}, respectively.






%Now we point out that $\nabla f = \nabla f_1 + f_2$ fails to be H{\"o}lder continuous for any $\alpha \in (0, 1]$.
%In the special case of $\bfv = 0$, it is clear that
%\begin{equation*}
%	\nabla f (\bfu) - \nabla f (\bfv) = \bfA \bfu + \nu \bfu_+^{1/2}.
%\end{equation*}
%We investigate the following two cases.
%
%\noindent {\bf Case I: } $\alpha \in (0, 1)$.
%Since $\norm{\bfA \bfu} \geq \norm{\bfA^{-1}}^{-1} \norm{\bfu}$, we have
%\begin{equation*}
%	\norm{\nabla f (\bfu) - \nabla f (\bfv)}
%	\geq \norm{\bfA \bfu} - \nu \norm{\bfu_+^{1/2}}
%	\geq \norm{\bfA^{-1}}^{-1} \norm{\bfu} - \nu n^{1/4} \norm{\bfu}^{1/2}.
%\end{equation*}
%For any $L > 0$, we choose $\bfu \in \Rn$ satisfying
%\begin{equation*}
%	\norm{\bfu} \geq \max \hkh{ 4 \nu^2 n^{1/2} \norm{\bfA^{-1}}^2, \, \dkh{ 2 L \norm{\bfA^{-1}} }^{1/ (1 - \alpha)} }.
%\end{equation*}
%Then a straightforward verification reveals that
%\begin{equation*}
%	\norm{\nabla f (\bfu) - \nabla f (\bfv)}
%	\geq \dfrac{1}{2} \norm{\bfA^{-1}}^{-1} \norm{\bfu}
%	\geq L \norm{\bfu}^{\alpha}.
%\end{equation*}
%
%\noindent {\bf Case II: } $\alpha = 1$.
%For all $\bfu \in \Rn_+$, we have
%\begin{equation*}
%	\norm{\nabla f (\bfu) - \nabla f (\bfv)}
%	\geq \nu \norm{\bfu_+^{1/2}} - \norm{\bfA \bfu}
%	\geq \nu \norm{\bfu}^{1/2} - \norm{\bfA} \norm{\bfu}.
%\end{equation*}
%For any $L > 0$, we choose $\bfu \in \Rn_+$ satisfying
%\begin{equation*}
%	\norm{\bfu} \leq \dfrac{\nu^2}{(L + \norm{\bfA})^2}.
%\end{equation*}
%Then it can be readily verified that
%\begin{equation*}
%	\norm{\nabla f (\bfu) - \nabla f (\bfv)}
%	\geq L \norm{\bfu}.
%\end{equation*}
%Therefore, $\nabla f$ cannot be H{\"o}lder continuous for all $\alpha \in (0, 1]$.
%




%\subsection{Related Works}

%The minimization of convex functions with globally $\alpha$-H{\"o}lder continuous gradients is first studied by Devolder et al. \cite{Devolder2014first} in the form of inexact oracles.
%Following this work, Lan \cite{Lan2015bundle} develops an accelerated prox-level method for solving the class of composite problems.
%Moreover, Nesterov \cite{Nesterov2015universal} proposes universal gradient methods for minimizing composite functions which have globally $\alpha$-H{\"o}lder continuous gradients of its smooth part.
%However, to the best of our knowledge, the complexity bound of gradient descent method \cref{eq:gd} for a function $f$ satisfying the locally $\alpha$-H{\"o}lder continuous condition with $0 < \alpha < 1$ has not been developed.
%In general, extending existing results to the local setting typically requires the gradient to be uniformly bounded or to satisfy other comparable assumptions.
%By contrast, our subsequent analysis proceeds without invoking any such additional conditions.


%\subsection{Contribution}


%The contribution of this paper is to provide a new complexity result for the gradient descent method \cref{eq:gd} with a fixed stepsize $\tau$ to solve strongly convex optimization problems on $\Rn$ when the gradient is locally $\alpha$-H{\"o}lder continuous with $0 < \alpha < 1$, but not Lipschitz continuous. We show that method \cref{eq:gd} with an appropriately chosen $\tau$ can  find a point $\bfu_k$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$ after at most $O (\log (\varepsilon^{-1})\varepsilon^{2\alpha-2})$ iterations.
%Under the same problem setting, the complexity bound derived in \cite{Devolder2014first}, formulated in terms of the average of iterates rather than the current iterate, is $O (\varepsilon^{-4 / (1 + \alpha)})$, which is inferior to ours by at least a factor of $O (\varepsilon^{-1})$.





\section{Vanilla Projected Gradient Descent Method with a Fixed Stepsize}

\label{sec:pgdm}

In this section, we attempt to employ the vanilla projected gradient descent method \cref{eq:gd} with a fixed stepsize to solve problem~\cref{opt:main}, whose complexity bound is also provided.
Example~\ref{exp:univ} illustrates that the projected gradient descent method \cref{eq:gd} with a fixed stepsize will experience stagnation before reaching the global minimizer.



To obtain an approximate solution to problem~\cref{opt:main}, it is necessary to choose a sufficiently small stepsize $\tau$ in the projected gradient descent method \cref{eq:gd}, the magnitude of which depends on the desired level of accuracy.
Let $M > 0$ be a constant defined as
\begin{equation}
	\label{eq:const-m}
	M = \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i)} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
\end{equation}
We select a specific stepsize $\tau = \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} / M$ in the projected gradient descent method, whose complete framework is presented in \cref{alg:gd}.
Two sequences $\{\bfv_k\}$ and $\{\bfu_k\}$ are maintained in \cref{alg:gd}, where $\bfv_k$ is generated by the projected gradient descent method and $\bfu_k$ corresponds to the iterate achieving the smallest objective function value among the first $k$ iterations.


\begin{algorithm2e}[ht]
	%\SetAlgoLined
	\caption{Projected Gradient Descent Method (PGDM).}
	\label{alg:gd}
	
	\KwIn{$\varepsilon > 0$.}
	
	Initialize $\bfu_0 = \bfv_0 \in \Omega$.
	
	Choose the stepsize $\tau = \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} / M$.
	
	\For{$k = 0, 1, 2, \dotsc$}{
		
		Compute
		\begin{equation*}
			\bfv_{k + 1} = \proj_{\Omega} \dkh{ \bfv_k - \tau \nabla f (\bfv_k) }.
		\end{equation*}
		
		Set
		\begin{equation*}
			\bfu_{k + 1} =
			\left\{
			\begin{aligned}
				& \bfv_{k + 1},
				&& \mbox{if~} f (\bfv_{k + 1}) \leq f (\bfu_k), \\
				& \bfu_k,
				&& \mbox{otherwise}.
			\end{aligned}
			\right.
		\end{equation*}
		
	}
	
	\KwOut{$\bfu_{k + 1}$.}
	
\end{algorithm2e}


Our subsequent analysis is based on the inexact oracle \cite{Devolder2014first} derived from the H{\"o}lder continuity condition of gradients, which is generalized to problem~\cref{opt:main} and demonstrated in the following proposition.


\begin{proposition}
	\label{prop:inexact}
	Suppose that Assumption~\ref{asp:function} holds.
	Let $\delta > 0$ and
	\begin{equation*}
		\rho \geq \max_{i \in [m]} \hkh{ \fkh{ \dfrac{1 - \alpha_i}{(1 + \alpha_i) \delta} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
	\end{equation*}
	Then for all $\bfu, \bfv \in \Omega$, we have
	\begin{equation*}
		f (\bfv) \leq f (\bfu) + \jkh{\nabla f (\bfu), \bfv - \bfu} + \dfrac{\rho}{2} \norm{\bfv - \bfu}^2 + \dfrac{\delta}{2}.
	\end{equation*}
\end{proposition}


\begin{proof}
	Since $\nabla f_i$ is H{\"o}lder continuous with an exponent $\alpha_i$, we can obtain that
	\begin{equation*}
		f_i (\bfv)
		\leq f_i (\bfu)
		+ \jkh{\nabla f_i (\bfu), \bfv - \bfu}
		+ \dfrac{L_i}{1 + \alpha_i} \norm{\bfv - \bfu}^{1 + \alpha_i},
	\end{equation*}
	for all $\bfu, \bfv \in \Omega$.
	Then, for each $i$, it follows from \cite[Lemma 2]{Nesterov2015universal} that
	\begin{equation*}
		f_i (\bfv)
		\leq f_i (\bfu)
		+ \jkh{\nabla f_i (\bfu), \bfv - \bfu}
		+ \dfrac{\rho}{2} \norm{\bfv - \bfu}^2 + \dfrac{\delta}{2}.
	\end{equation*}
	Summing the above relationship over $i \in [m]$, we immediately arrive at the assertion of this proposition.
	The proof is completed.
\end{proof}


Now, we are able to derive the complexity bound of \cref{alg:gd} in the following theorem.


\begin{theorem}
	\label{thm:gd}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then after at most
	\begin{equation*}
		O \dkh{ \log \dkh{\dfrac{1}{\varepsilon}} \dfrac{1}{ \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} } }
	\end{equation*}
	iterations, \cref{alg:gd} will find an iterate $\bfu_k \in \Omega$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
\end{theorem}


\begin{proof}
	In view of \cref{prop:inexact}, we take
	\begin{equation*}
		\rho
		= \dfrac{1}{\tau}
		= \dfrac{M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}
		\geq \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
	\end{equation*}
	Then it holds that
	\begin{equation*}
		f (\bfv_{k + 1})
		\leq f (\bfv_k) + \jkh{\nabla f (\bfv_k), \bfv_{k + 1} - \bfv_k} + \dfrac{1}{2 \tau} \norm{\bfv_{k + 1} - \bfv_k}^2 + \dfrac{\mu \varepsilon^2}{4},
	\end{equation*}
	which, after a suitable rearrangement, can be equivalently written as
	\begin{equation}
		\label{eq:nabla-gd}
		\jkh{\nabla f (\bfv_k), \bfv_k - \bfv_{k + 1}}
		\leq f (\bfv_k) - f (\bfv_{k + 1})
		+ \dfrac{\mu \varepsilon^2}{4}
		+ \dfrac{1}{2 \tau} \norm{\bfv_{k + 1} - \bfv_k}^2.
	\end{equation}
	Recall that $f\uast = f (\bfu\uast)$.
	By virtue of the strong convexity of $f$, we can obtain that
	%	\begin{equation*}
		%		f\uast
		%		= f (\bfu\uast)
		%		\geq f (\bfu_k)
		%		+ \nabla f (\bfu_k)\zz (\bfu\uast - \bfu_k)
		%		+ \dfrac{\mu}{2} \norm{\bfu_k - \bfu\uast}^2,
		%	\end{equation*}
	\begin{equation}
		\label{eq:mu-grad}
		\jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_k}
		\leq f\uast - f (\bfv_k) - \dfrac{\mu}{2} \norm{\bfv_k - \bfu\uast}^2.
	\end{equation}
	The optimality condition of the projection problem defining $\bfv_{k + 1}$ yields that
	\begin{equation*}
		\jkh{\bfv_{k + 1} - \bfv_k + \tau \nabla f (\bfv_k), \bfu - \bfv_{k + 1}} \geq 0,
	\end{equation*}
	for all $\bfu \in \Omega$.
	Upon taking $\bfu = \bfu\uast$, we have
	\begin{equation*}
		\begin{aligned}
			\jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			\leq {} & \tau \jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_{k + 1}} \\
			= {} & \tau \jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_k}
			+ \tau \jkh{\nabla f (\bfv_k), \bfv_k - \bfv_{k + 1}},
		\end{aligned}
	\end{equation*}
	which together with \cref{eq:nabla-gd} and \cref{eq:mu-grad} implies that
	\begin{equation*}
		\begin{aligned}
			\jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			\leq {} & \tau \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} } - \dfrac{\mu \tau}{2} \norm{\bfv_k - \bfu\uast}^2 \\
			& + \dfrac{1}{2} \norm{\bfv_{k + 1} - \bfv_k}^2.
		\end{aligned}
	\end{equation*}
	Moreover, it can be readily verified that
	\begin{equation}
		\label{eq:vk-uast}
		\begin{aligned}
			\norm{\bfv_{k + 1} - \bfu\uast}^2
			= {} & \norm{\bfv_{k + 1} - \bfv_k + \bfv_k - \bfu\uast}^2 \\
			= {} & \norm{\bfv_k - \bfu\uast}^2
			+ 2 \jkh{\bfv_{k + 1} - \bfv_k, \bfv_k - \bfu\uast}
			+ \norm{\bfv_{k + 1} - \bfv_k}^2 \\
			= {} & \norm{\bfv_k - \bfu\uast}^2
			+ 2 \jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			- \norm{\bfv_{k + 1} - \bfv_k}^2.
		\end{aligned}
	\end{equation}
	Collecting the above two relationships together, we arrive at
	\begin{equation*}
		\norm{\bfv_{k + 1} - \bfu\uast}^2
		\leq \dkh{1 - \mu \tau} \norm{\bfv_k - \bfu\uast}^2
		+ 2 \tau \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} }.
	\end{equation*}
	From the construction of $\bfu_k$ in \cref{alg:gd}, it then follows that $f (\bfv_l) \geq f (\bfu_k)$ for all $l \in \{1, 2, \dotsc, k\}$.
	Let $C_k = \sum_{l = 1}^{k} \dkh{1 - \mu \tau}^{l - 1}$ be a constant.
	Applying the above relationship recursively for $k$ times leads to that
	\begin{equation*}
		\begin{aligned}
			\norm{\bfv_k - \bfu\uast}^2
			\leq {} & \dkh{ 1 - \mu \tau }^k \norm{\bfu_0 - \bfu\uast}^2
			+ 2 \tau \sum_{l = 1}^{k} \dkh{1 - \mu \tau}^{l - 1} \dkh{ f\uast - f (\bfv_l) + \dfrac{\mu \varepsilon^2}{4} } \\
			\leq {} & \dkh{ 1 - \mu \tau }^k \norm{\bfu_0 - \bfu\uast}^2
			+ 2 \tau \dkh{ f\uast - f (\bfu_k) + \dfrac{\mu \varepsilon^2}{4} } C_k,
		\end{aligned}
	\end{equation*}
	which together with $\norm{\bfv_k - \bfu\uast} \geq 0$ and $C_k \geq 1$ implies that
	\begin{equation*}
		f (\bfu_k) - f\uast
		\leq \dfrac{\dkh{ 1 - \mu \tau }^k}{2 \tau C_k} \norm{\bfu_0 - \bfu\uast}^2
		+ \dfrac{\mu \varepsilon^2}{4}
		\leq \dfrac{\dkh{ 1 - \mu \tau }^k}{2 \tau} \norm{\bfu_0 - \bfu\uast}^2
		+ \dfrac{\mu \varepsilon^2}{4}.
	\end{equation*}
	According to the strong convexity of $f$ and the optimality condition of problem~\cref{opt:main}, we have
	\begin{equation}
		\label{eq:dist}
		f (\bfu_k) - f\uast
		\geq \jkh{\nabla f (\bfu\uast), \bfu_k - \bfu\uast}
		+ \dfrac{\mu}{2} \norm{\bfu_k - \bfu\uast}^2
		\geq \dfrac{\mu}{2} \norm{\bfu_k - \bfu\uast}^2.
	\end{equation}
	Hence, it holds that
	\begin{equation*}
		\begin{aligned}
			\norm{\bfu_k - \bfu\uast}^2
			\leq {} & \dfrac{2}{\mu} \dkh{f (\bfu_k) - f\uast}
			\leq \dfrac{\dkh{ 1 - \mu \tau }^k}{\mu \tau} \norm{\bfu_0 - \bfu\uast}^2 + \dfrac{\varepsilon^2}{2} \\
			\leq {} & \dfrac{M \norm{\bfu_0 - \bfu\uast}^2}{\mu \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \dkh{ 1 - \dfrac{\mu}{M} \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} }^k + \dfrac{\varepsilon^2}{2}.
		\end{aligned}
	\end{equation*}
	We denote by $K\uast_\varepsilon$ the smallest iteration number $k$ such that $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
	Then solving the inequality $M \norm{\bfu_0 - \bfu\uast}^2 \varepsilon^{- 2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} ( 1 - \mu \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} / M )^k / \mu \leq \varepsilon^2 / 2$ indicates that
	\begin{equation*}
		\begin{aligned}
			K\uast_\varepsilon
			\leq {} & \dfrac{ 4 \log ( (2 M \norm{\bfu_0 - \bfu\uast}^2 / \mu)^{(1 + \hat{\alpha}) / 4} / \varepsilon ) }{ - \log ( 1 - \mu \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})} / M ) (1 + \hat{\alpha}) } \\
			\leq {} & \dfrac{4 M \log ( (2 M \norm{\bfu_0 - \bfu\uast}^2 / \mu)^{(1 + \hat{\alpha}) / 4} / \varepsilon )}{\mu (1 + \hat{\alpha}) \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}.
		\end{aligned}
	\end{equation*}
	The proof is completed.
\end{proof}


\Cref{thm:gd} demonstrates that the iteration complexity of \cref{alg:gd} with a fixed stepsize is $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + \hat{\alpha})})$ for problem~\cref{opt:main}.
This complexity result generalizes the classical linear convergence when $\hat{\alpha} = 1$, which highlights the performance degradation incurred by non-Lipschitz gradients.





\section{Universal Primal Gradient Method}

\label{sec:upgm}


The fixed stepsize $\tau$ chosen in \cref{alg:gd} depends on the parameters $\alpha_i$ and $L_i$ for all $i \in [m]$, which are often unknown and hard to estimate in practice.
%certain problem-specific parameters of problem~\cref{opt:main}, including
To address this issue, we adopt the universal primal gradient method (UPGM) proposed by Nesterov \cite{Nesterov2015universal} to solve problem~\cref{opt:main}.
This method incorporates a line-search procedure to adaptively determine the stepsize at each iteration, and its overall framework is outlined in \cref{alg:upgm}.


\begin{algorithm2e}[ht]
	%\SetAlgoLined
	\caption{Universal Primal Gradient Method (UPGM).}
	\label{alg:upgm}
	
	\KwIn{$\varepsilon > 0$.}
	
	Initialize $\bfu_0 = \bfv_0 \in \Omega$ and $\rho_0 > 0$.
	
	\For{$k = 0, 1, 2, \dotsc$}{
		
		\For{$j_k = 0, 1, 2, \dotsc$}{
			
			Compute
			\begin{equation*}
				\bfv_{k + 1} = \proj_{\Omega} \dkh{ \bfv_k - \dfrac{1}{2^{j_k} \rho_k} \nabla f (\bfv_k) }.
			\end{equation*}
			
			{\bf If} $\bfv_{k + 1}$ satisfies the following line-search condition,
			\begin{equation}
				\label{eq:line-upgm}
				\begin{aligned}
					f (\bfv_{k + 1})
					\leq {} & f (\bfv_k)
					+ \jkh{\nabla f (\bfv_k), \bfv_{k + 1} - \bfv_k} \\
					& + \dfrac{2^{j_k} \rho_k}{2} \norm{\bfv_{k + 1} - \bfv_k}^2
					+ \dfrac{\mu \varepsilon^2}{4},
				\end{aligned}
			\end{equation}
			
			{\bf then}	break.
			
		}
		
		Update $\rho_{k + 1} = 2^{j_k} \rho_k$.
		
		Set
		\begin{equation*}
			\bfu_{k + 1} =
			\left\{
			\begin{aligned}
				& \bfv_{k + 1},
				&& \mbox{if~} f (\bfv_{k + 1}) \leq f (\bfu_k), \\
				& \bfu_k,
				&& \mbox{otherwise}.
			\end{aligned}
			\right.
		\end{equation*}
		
	}
	
	\KwOut{$\bfu_{k + 1}$.}
	
\end{algorithm2e}


Next, we establish the iteration complexity of \cref{alg:upgm}, which remains on the same order as that of the projected gradient descent method with a fixed stepsize.


\begin{theorem}
	\label{thm:upgm}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then after at most
	\begin{equation*}
		O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \dfrac{1}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} }
	\end{equation*}
	iterations, \cref{alg:upgm} will attain an iterate $\bfu_k \in \Omega$ satisfying that $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
\end{theorem}


\begin{proof}
	Obviously, there exists $j_k \in \bN$ such that
	\begin{equation*}
		2^{j_k} \rho_k \geq \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
	\end{equation*}
	By invoking the results of \cref{prop:inexact}, we know that condition~\cref{eq:line-upgm} is satisfied.
	Hence, the line-search step in \cref{alg:upgm} can be terminated after a finite number of trials and the required number of trials $j_k$ satisfies
	\begin{equation}
		\label{eq:jk-upgm}
		2^{j_k} \rho_k
		\leq 2 \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }
		\leq \dfrac{2 M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}},
	\end{equation}
	where $M > 0$ is a constant defined in \cref{eq:const-m}.
	Moreover, the line-search condition \cref{eq:line-upgm} directly yields that
	\begin{equation}
		\label{eq:nabla-upgm}
		\jkh{ \nabla f (\bfv_k), \bfv_k - \bfv_{k + 1} }
		\leq f (\bfv_k) - f (\bfv_{k + 1})
		+ \dfrac{2^{j_k} \rho_k}{2} \norm{\bfv_{k + 1} - \bfv_k}^2
		+ \dfrac{\mu \varepsilon^2}{4}.
	\end{equation}
	According to the optimality condition of the projection problem defining $\bfv_{k + 1}$, we have
	\begin{equation*}
		\jkh{\bfv_{k + 1} - \bfv_k + \dfrac{1}{2^{j_k} \rho_k} \nabla f (\bfv_k), \bfu\uast - \bfv_{k + 1}} \geq 0,
	\end{equation*}
	which further implies that
	\begin{equation*}
		\begin{aligned}
			\jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			\leq {} & \dfrac{1}{2^{j_k} \rho_k} \jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_{k + 1}} \\
			\leq {} & \dfrac{1}{2^{j_k} \rho_k} \jkh{\nabla f (\bfv_k), \bfu\uast - \bfv_k}
			+ \dfrac{1}{2^{j_k} \rho_k} \jkh{\nabla f (\bfv_k), \bfv_k - \bfv_{k + 1}}.
		\end{aligned}
	\end{equation*}
	Substituting \cref{eq:mu-grad} and \cref{eq:nabla-upgm} into the above relationship leads to that
	\begin{equation*}
		\begin{aligned}
			\jkh{\bfv_{k + 1} - \bfv_k, \bfv_{k + 1} - \bfu\uast}
			\leq {} & \dfrac{1}{2^{j_k} \rho_k} \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} } \\
			& + \dfrac{1}{2} \norm{\bfv_{k + 1} - \bfv_k}^2
			- \dfrac{\mu}{2^{j_k + 1} \rho_k} \norm{\bfv_k - \bfu\uast}^2,
		\end{aligned}
	\end{equation*}
	Thus, it follows from relationship~\cref{eq:vk-uast} that
	\begin{equation*}
		\begin{aligned}
			\norm{\bfv_{k + 1} - \bfu\uast}^2
			\leq {} & \dkh{1 - \dfrac{\mu}{2^{j_k} \rho_k}} \norm{\bfv_k - \bfu\uast}^2
			+ \dfrac{2}{2^{j_k} \rho_k} \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} } \\
			\leq {} & \dkh{1 - \dfrac{\mu \varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}{2 M}} \norm{\bfv_k - \bfu\uast}^2
			+ \dfrac{2}{\rho_0} \dkh{ f\uast - f (\bfv_{k + 1}) + \dfrac{\mu \varepsilon^2}{4} },
		\end{aligned}
	\end{equation*}
	where the last inequality comes from \cref{eq:jk-upgm} and $2^{j_k} \rho_k \geq \rho_0$.
	The remaining part of the proof follows the same line of reasoning as that of \cref{thm:gd} and is therefore omitted here for the sake of brevity.
\end{proof}


We end this section by estimating the total number of line-search steps required by \cref{alg:upgm}.


\begin{corollary}
	\label{coro:upgm}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then \cref{alg:upgm} requires at most
	\begin{equation*}
		O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \dfrac{1}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} }
	\end{equation*}
	line-search steps for the generated sequence $\{\bfu_k\}$ to satisfy $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
\end{corollary}


\begin{proof}
	Let $N_k$ be the total number of line-search steps after $k$ iterations in \cref{alg:upgm}.
	From the update rule $\rho_{k + 1} = 2^{j_k} \rho_k$, we can obtain that $j_k = \log \rho_{k + 1} - \log \rho_k$.
	Then a straightforward verification reveals that
	\begin{equation}
		\label{eq:nk}
		N_k = \sum_{l = 0}^{k} (j_l + 1)
		= k + 1 + \log \rho_{k + 1} - \log \rho_0,
	\end{equation}
	which together with relationship~\cref{eq:jk-upgm} implies that
	\begin{equation*}
		\begin{aligned}
			N_k
			\leq{} & k + 1 + \log \dkh{ \dfrac{2 M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} } - \log \rho_0 \\
			\leq {} & k
			+ \dfrac{2 (1 - \hat{\alpha})}{1 + \hat{\alpha}} \log \dkh{\dfrac{1}{\varepsilon}}
			+ \log \dkh{ \dfrac{2 M}{\rho_0} }
			+ 1.
		\end{aligned}
	\end{equation*}
	By invoking the results of \cref{thm:upgm}, we conclude that \cref{alg:upgm} requires at most $O ( \log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + \hat{\alpha})} )$ line-search steps, which completes the proof.
\end{proof}


At each iteration of \cref{alg:upgm}, we evaluate both the function value and the gradient at $\bfv_k$.
In addition, an extra function evaluation at $\bfv_{k + 1, j_k}$ is involved during each line-search step.
Therefore, \cref{thm:upgm} and \cref{coro:upgm} together reveal that the total number of function and gradient evaluations required by \cref{alg:upgm} is $O ( \log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + \hat{\alpha})} )$.





\section{Universal Fast Gradient Method}

\label{sec:ufgm}


To obtain a sharper complexity bound, we devise in this section a universal fast gradient method (UFGM) tailored to problem~\cref{opt:main}.
The proposed scheme, summarized in \cref{alg:ufgm}, exhibits slight but essential differences from the algorithm introduced by Nesterov \cite{Nesterov2015universal} to exploit the strong convexity of the objective function.


\begin{algorithm2e}[ht]
	%\SetAlgoLined
	\caption{Universal Fast Gradient Method (UFGM).}
	\label{alg:ufgm}
	
	\KwIn{$\varepsilon > 0$.}
	
	Initialize $\bfu_0 = \bfw_0 \in \Omega$ and $\rho_0 \geq \mu$.
	
	\For{$k = 0, 1, 2, \dotsc$}{
		
		\For{$j_k = 0, 1, 2, \dotsc$}{
			
			Set $\nu_k = \sqrt{ \mu / (2^{j_k} \rho_k) }$ and $\eta_k = \nu_k / (1 + \nu_k)$.
			
			Compute
			\begin{equation}
				\label{eq:vk}
				\bfv_k = (1 - \eta_k) \bfu_k + \eta_k \proj_{\Omega} (\bfw_k),
			\end{equation}
			and
			\begin{equation}
				\label{eq:zk}
				\bfz_k = \proj_{\Omega} \dkh{ \proj_{\Omega} (\bfw_k) - \dfrac{\nu_k}{\mu} \nabla f (\bfv_k) }.
			\end{equation}
			
			Set
			\begin{equation}
				\label{eq:uk+1}
				\bfu_{k + 1} = (1 - \eta_k) \bfu_k + \eta_k \bfz_k.
			\end{equation}
			
			{\bf If} $\bfu_{k + 1}$ satisfies the following line-search condition,
			%		\begin{equation}
				%			\label{eq:local}
				%			\norm{\bfu_{k + 1} - \bfv_k} \leq \gamma,
				%		\end{equation}
			%		and
			\begin{equation}
				\label{eq:line}
				\begin{aligned}
					f (\bfu_{k + 1})
					\leq {} & f (\bfv_k)
					+ \jkh{\nabla f (\bfv_k), \bfu_{k + 1} - \bfv_k} \\
					& + \dfrac{\mu}{2 \nu_k^2} \norm{\bfu_{k + 1} - \bfv_k}^2
					+ \dfrac{\eta_k \mu \varepsilon^2}{4},
				\end{aligned}
			\end{equation}
			
			{\bf then}	break.
			
		}
		
		Set $\rho_{k + 1} = 2^{j_k} \rho_k$ and update $\bfw_{k + 1}$ by
		\begin{equation}
			\label{eq:wk+1}
			\bfw_{k + 1} = (1 - \eta_k) \bfw_k + \eta_k \bfv_k - \dfrac{\eta_k}{\mu} \nabla f (\bfv_k).
		\end{equation}
		
	}
	
	\KwOut{$\bfu_{k + 1}$.}
	
\end{algorithm2e}

%\begin{equation*}
%	\delta = \dfrac{\mu \varepsilon^2}{2}
%\end{equation*}

The following lemma illustrates that the line-search process in \cref{eq:line} is well-defined, which is guaranteed to terminate in a finite number of trials.


\begin{lemma}
	\label{le:line-ufgm}
	There exists an integer $j_k \in \bN$ such that the line-search condition \cref{eq:line} is satisfied in \cref{alg:ufgm}.
\end{lemma}


\begin{proof}
	It follows from the definition of $\eta_k$ and $\nu_k \leq 1$ that
	\begin{equation*}
		\eta_k
		= \dfrac{\nu_k}{1 + \nu_k}
		\geq \dfrac{\nu_k}{2},
		\quad\mbox{and}\quad
		\dfrac{\mu}{\nu_k^2}
		= 2^{j_k} \rho_k.
	\end{equation*}
	Recall that $\hat{\alpha} = \min_{i \in [m]} \alpha_i \in (0, 1]$.
	Then we have
	\begin{equation*}
		\begin{aligned}
			\dfrac{\mu}{\nu_k^2} \eta_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
			\geq {} & \dfrac{2^{j_k} \rho_k}{2^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \nu_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \\
			= {} &  \dfrac{2^{j_k} \rho_k}{2^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \fkh{ \dfrac{\mu}{2^{j_k} \rho_k} }^{(1 - \hat{\alpha}) / (2 (1 + \hat{\alpha}))} \\
			= {} & \dfrac{\mu^{(1 - \hat{\alpha}) / (2 (1 + \hat{\alpha}))}}{2^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \fkh{ 2^{j_k} \rho_k }^{(1 + 3 \hat{\alpha}) / (2 (1 + \hat{\alpha}))},
		\end{aligned}
	\end{equation*}
	where the first equality comes from the definition of $\nu_k$.
	Now it is clear that
	\begin{equation*}
		\dfrac{\mu}{\nu_k^2} \eta_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \to \infty,
	\end{equation*}
	as $j_k \to \infty$.
	Thus, there exists $j_k \in \bN$ such that
	\begin{equation}
		\label{eq:stepsize}
		\dfrac{\mu}{\nu_k^2} \eta_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
		\geq \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} },
	\end{equation}
	which further implies that
	\begin{equation*}
		\begin{aligned}
			\dfrac{\mu}{\nu_k^2}
			\geq {} & \dfrac{1}{\eta_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} } \\
			\geq {} & \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\eta_k \mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }.
		\end{aligned}
	\end{equation*}
	As a direct consequence of \cref{prop:inexact}, we can proceed to show that the line-search condition \cref{eq:line} is satisfied, which completes the proof.
\end{proof}


\begin{remark}
	When the parameters of problem \cref{opt:main} are fully specified, \cref{alg:ufgm} may alternatively be implemented with a fixed stepsize. 
	Recall that $M > 0$ is a constant defined in \cref{eq:const-m}. 
	By invoking the result of \cref{le:line-ufgm}, we can fix 
	\begin{equation*}
		\nu_k = 2 \fkh{ \dfrac{\mu}{4 M} }^{(1 + \hat{\alpha}) / (1 + 3 \hat{\alpha})} \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})},
	\end{equation*}
	and dispense with the parameter $\rho_k$ and the line-search procedure in \cref{eq:line}. 
	Under this choice, \cref{alg:ufgm} continues to enjoy the same iteration complexity established later. 
\end{remark}


We now introduce the estimating sequences associated with \cref{alg:ufgm}, which play a crucial role in our subsequent analysis. 


\begin{lemma}
	\label{le:phi}
	Let $\{\sigma_k\}$ be a sequence of positive constants defined recursively by
	\begin{equation}
		\label{eq:sigma}
		\sigma_{k + 1} = (1 + \nu_k ) \sigma_k,
	\end{equation}
	with $\sigma_0 = 1$.
	And let $\{\phi_k\}$ be a sequence of functions defined recursively by
	\begin{equation}
		\label{eq:phi}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			= {} & \phi_k (\bfu)
			- \nu_k \sigma_k f\uast
			+ \nu_k \sigma_k f (\bfv_k)
			+ \nu_k \sigma_k \jkh{\nabla f (\bfv_k), \bfu - \bfv_k} \\
			& + \dfrac{\nu_k \sigma_k \mu}{2} \norm{\bfu - \bfv_k}^2,
		\end{aligned}
	\end{equation}
	with $\phi_0 (\bfu) = c_0 + \sigma_0 \mu \norm{\bfu - \bfw_0}^2 / 2$ for $c_0 = f (\bfu_0) - f\uast - \mu \varepsilon^2 / 4$ and $\bfw_0 \in \Omega$.
	Then, for all $k \in \bN$, the function $\phi_k$ preserves the following canonical form,
	\begin{equation}
		\label{eq:phi-can}
		\phi_k (\bfu) = c_k + \dfrac{\sigma_k \mu}{2} \norm{\bfu - \bfw_k}^2,
	\end{equation}
	where $\{c_k\}$ is a sequence of real numbers and $\{\bfw_k\}$ is defined recursively by \cref{eq:wk+1}.
\end{lemma}


\begin{proof}
	We first prove that $\nabla^2 \phi_k = \sigma_k \mu I$ for all $k \in \bN$ by induction.
	It is evident that $\nabla^2 \phi_0 = \sigma_0 \mu I$.
	Now we assume that $\nabla^2 \phi_k = \sigma_k \mu I$ for some $k$.
	Then relationships \cref{eq:sigma} and \cref{eq:phi} imply that
	\begin{equation*}
		\nabla^2 \phi_{k + 1}
		= \nabla^2 \phi_k + \nu_k \sigma_k \mu I
		= \sigma_k \mu I + \nu_k \sigma_k \mu I
		= \sigma_{k + 1} \mu I.
	\end{equation*}
	Thus, we know that $\nabla^2 \phi_k = \sigma_k \mu I$ for all $k \in \bN$, which, in turn, justifies the canonical form of $\phi_k$ in \cref{eq:phi-can}.
	
	Next, by combining two relationships \cref{eq:phi} and \cref{eq:phi-can} together, we can obtain that
	\begin{equation*}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			= {} & c_k
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfu - \bfw_k}^2
			- \nu_k \sigma_k f\uast
			+ \nu_k \sigma_k f (\bfv_k) \\
			& + \nu_k \sigma_k \jkh{\nabla f (\bfv_k), \bfu - \bfv_k}
			+ \dfrac{\nu_k \sigma_k \mu}{2} \norm{\bfu - \bfv_k}^2.
		\end{aligned}
	\end{equation*}
	Since $\bfw_{k + 1}$ is a global minimizer of $\phi_{k + 1}$ over $\Rn$, the first-order optimality condition yields that
	\begin{equation*}
		\begin{aligned}
			0 = \nabla \phi_{k + 1} (\bfw_{k + 1})
			= {} & \sigma_k \mu (\bfw_{k + 1} - \bfw_k)
			+ \nu_k \sigma_k \nabla f (\bfv_k)
			+ \nu_k \sigma_k \mu (\bfw_{k + 1} - \bfv_k) \\
			= {} & (1 + \nu_k) \sigma_k \mu \bfw_{k + 1}
			- \sigma_k \mu \bfw_k
			- \nu_k \sigma_k \mu \bfv_k
			+ \nu_k \sigma_k \nabla f (\bfv_k),
		\end{aligned}
	\end{equation*}
	from which the closed-form expression of $\bfw_{k + 1}$ in \cref{eq:wk+1} can be derived.
	The proof is completed.
\end{proof}


The following lemma characterizes the relationship between the objective function of problem \cref{opt:main} and the estimating sequences. 


\begin{lemma}
	Let $\sigma_k$ and $\{\phi_k\}$ be the sequences defined in \cref{le:phi}.
	Then we have
	\begin{equation}
		\label{eq:estimating}
		\phi_k (\bfu) \leq \sigma_k ( f (\bfu) - f\uast ) + \phi_0 (\bfu),
	\end{equation}
	for all $\bfu \in \Omega$ and $k \in \bN$.
\end{lemma}


\begin{proof}
	We prove that $\{\phi_k\}$ and $\{\sigma_k\}$ satisfy relationship~\cref{eq:estimating} by induction.
	It is obvious that \cref{eq:estimating} holds for $k = 0$ since $f (\bfu) \geq f\uast$ for any $\bfu \in \Omega$.
	Now we assume that \cref{eq:estimating} holds for some $k \in \bN$.
	It follows from the strong convexity of $f$ that
	\begin{equation*}
		f (\bfu) \geq f (\bfv_k) + \jkh{\nabla f (\bfv_k), \bfu - \bfv_k} + \frac{\mu}{2} \norm{\bfu - \bfv_k}^2,
	\end{equation*}
	for all $\bfu \in \Omega$.
	Then substituting the above relationship into \cref{eq:phi} leads to that
	\begin{equation*}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			\leq {} & \phi_k (\bfu)
			- \nu_k \sigma_k f\uast
			+ \nu_k \sigma_k f (\bfu) \\
			\leq {} & \sigma_k ( f (\bfu) - f\uast ) + \phi_0 (\bfu)
			+ \nu_k \sigma_k (f (\bfu) - f\uast) \\
			= {} & \sigma_{k + 1} (f (\bfu) -f\uast) + \phi_0 (\bfu),
		\end{aligned}
	\end{equation*}
	which indicates that \cref{eq:estimating} also holds for $k + 1$.
	We complete the proof.
\end{proof}


Next, we proceed to show that the function value error of \cref{alg:ufgm} is controlled by the estimating sequences. 


\begin{proposition}
	Let $\{\sigma_k\}$ and $\{\phi_k\}$ be the sequences defined in \cref{le:phi}.
	Then the sequence $\{\bfu_k\}$ generated by \cref{alg:ufgm} satisfies
	\begin{equation}
		\label{eq:error}
		f (\bfu_k) - f\uast \leq \dfrac{1}{\sigma_k} \phi_0 (\bfu\uast) + \dfrac{\mu \varepsilon^2}{4},
	\end{equation}
	for all $k \in \bN$. 
\end{proposition}


\begin{proof}
	Let $\phi_k\uast := \min_{\bfu \in \Omega} \phi_k (\bfu)$. 
	We first prove by induction that
	\begin{equation}
		\label{eq:descent}
		\sigma_k \dkh{ f (\bfu_k) - f\uast - \dfrac{\mu \varepsilon^2}{4} } \leq \phi_k\uast,
	\end{equation}
	for any $k \in \bN$. 
	It is clear that \cref{eq:descent} holds for $k = 0$ since $\sigma_0 = 1$ and $\phi_0\uast = \phi_0 (\bfw_0) = f (\bfu_0) - f\uast - \mu \varepsilon^2 / 4$.
	Now we assume that \cref{eq:descent} holds for some $k \in \bN$ and investigate the situation for $k + 1$.
	
	From the canonical form \cref{eq:phi-can}, it follows that $\phi_k$ is a strongly convex function and $\proj_{\Omega} (\bfw_k) = \argmin_{\bfu \in \Omega} \phi_k (\bfu)$.
	By invoking the result of \cite[Corollary 2.2.1]{Nesterov2018lectures}, we have
	\begin{equation*}
		\begin{aligned}
			\phi_k (\bfu)
			\geq {} & \phi_k\uast + \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2 \\
			\geq {} & \sigma_k \dkh{ f (\bfu_k) - f\uast - \dfrac{\mu \varepsilon^2}{4} } + \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2,
		\end{aligned}
	\end{equation*}
	for all $\bfu \in \Omega$.
	Then relationship~\cref{eq:phi} yields that
	\begin{equation*}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			\geq {} & \sigma_k \dkh{ f (\bfu_k) - f\uast - \dfrac{\mu \varepsilon^2}{4} }
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2
			- \nu_k \sigma_k f\uast \\
			& + \nu_k \sigma_k f (\bfv_k)
			+ \nu_k \sigma_k \jkh{\nabla f (\bfv_k), \bfu - \bfv_k}
			+ \dfrac{\nu_k \sigma_k \mu}{2} \norm{\bfu - \bfv_k}^2 \\
			\geq {} & \sigma_{k + 1} \dkh{f (\bfv_k) - f\uast}
			- \dfrac{\sigma_k \mu \varepsilon^2}{4}
			+ \jkh{\nabla f (\bfv_k), \sigma_k\bfu_k - \sigma_{k + 1} \bfv_k} \\
			& + \nu_k \sigma_k \jkh{\nabla f (\bfv_k), \bfu}
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2 \\
			= {} & \sigma_{k + 1} \dkh{f (\bfv_k) - f\uast}
			- \dfrac{\sigma_k \mu \varepsilon^2}{4}
			+ \nu_k \sigma_k \jkh{\nabla f (\bfv_k), \bfu - \proj_{\Omega} (\bfw_k)} \\
			& + \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2,
		\end{aligned}
	\end{equation*}
	where the second inequality comes from the strong convexity of $f$ and \cref{eq:sigma}, and the last equality holds due to the definition of $\bfv_k$ in \cref{eq:vk}.
	According to the definition of $\bfz_k$ in \cref{eq:zk}, we can obtain that
	\begin{equation*}
		\begin{aligned}
			& \nu_k \sigma_k \jkh{\nabla f (\bfv_k), \bfu - \proj_{\Omega} (\bfw_k)}
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfu - \proj_{\Omega} (\bfw_k)}^2 \\
			= {} & \dfrac{\sigma_k \mu}{2} \norm{ \bfu - \dkh{ \proj_{\Omega} (\bfw_k) - \dfrac{\nu_k}{\mu} \nabla f (\bfv_k) } }^2
			- \dfrac{\nu_k^2 \sigma_k}{2 \mu} \norm{\nabla f (\bfv_k)}^2 \\
			\geq {} & \dfrac{\sigma_k \mu}{2} \norm{ \bfz_k - \dkh{ \proj_{\Omega} (\bfw_k) - \dfrac{\nu_k}{\mu} \nabla f (\bfv_k) } }^2
			- \dfrac{\nu_k^2 \sigma_k}{2 \mu} \norm{\nabla f (\bfv_k)}^2 \\
			= {} & \nu_k \sigma_k \jkh{\nabla f (\bfv_k), \bfz_k - \proj_{\Omega} (\bfw_k)}
			+ \dfrac{\sigma_k \mu}{2} \norm{\bfz_k - \proj_{\Omega} (\bfw_k)}^2.
		\end{aligned}
	\end{equation*}
	As a result, it holds that
	\begin{equation}
		\label{eq:phi-u}
		\begin{aligned}
			\phi_{k + 1} (\bfu)
			\geq {} & \sigma_{k + 1} \dkh{ f (\bfv_k) - f\uast }
			- \dfrac{\sigma_k \mu \varepsilon^2}{4}
			+ \nu_k \sigma_k \jkh{\nabla f (\bfv_k), \bfz_k - \proj_{\Omega} (\bfw_k)} \\
			& + \dfrac{\sigma_k \mu}{2} \norm{\bfz_k - \proj_{\Omega} (\bfw_k)}^2,
		\end{aligned}
	\end{equation}
	for all $\bfu \in \Omega$.
	From the definitions of $\bfv_k$ and $\bfu_{k + 1}$ in \cref{eq:vk} and \cref{eq:uk+1}, it can be derived that $\bfz_k - \proj_{\Omega} (\bfw_k) = (\bfu_{k + 1} - \bfv_k) / \eta_k$.
	Substituting this relationship into \cref{eq:phi-u} and taking $\bfu = \proj_{\Omega} (\bfw_{k + 1})$, we arrive at
	\begin{equation*}
		\dfrac{\phi_{k + 1}\uast}{\sigma_{k + 1}}
		\geq f (\bfv_k) - f\uast + \jkh{\nabla f (\bfv_k), \bfu_{k + 1} - \bfv_k}
		+ \dfrac{\mu}{2 \nu_k^2} \norm{\bfu_{k + 1} - \bfv_k}^2
		- \dfrac{(1 - \eta_k) \mu \varepsilon^2}{4},
	\end{equation*}
	which together with the line-search condition \cref{eq:line} implies that
	\begin{equation*}
		\begin{aligned}
			\dfrac{\phi_{k + 1}\uast}{\sigma_{k + 1}}
			\geq f (\bfu_{k + 1})
			- f\uast
			- \dfrac{\eta_k \mu \varepsilon^2}{4}
			- \dfrac{(1 - \eta_k) \mu \varepsilon^2}{4}
			= f (\bfu_{k + 1})
			- f\uast
			- \dfrac{\mu \varepsilon^2}{4}.
		\end{aligned}
	\end{equation*}
	Therefore, relationship~\cref{eq:descent} also holds for $k + 1$.
	
	Finally, by collecting two relationships \cref{eq:estimating} and \cref{eq:descent} together, we can obtain that
	\begin{equation*}
		\begin{aligned}
			\sigma_k \dkh{ f (\bfu_k) - f\uast - \dfrac{\mu \varepsilon^2}{4} }
			\leq {} & \min_{\bfu \in \Omega} \phi_k (\bfu)
			\leq \min_{\bfu \in \Omega} \hkh{ \sigma_k (f (\bfu) - f\uast) + \phi_0 (\bfu) } \\
			\leq {} & \sigma_k (f (\bfu\uast) - f\uast) + \phi_0 (\bfu\uast) \\
			= {} & \phi_0 (\bfu\uast),
		\end{aligned}
	\end{equation*}
	which completes the proof.
\end{proof}


With the above preparatory results in place, we are now in a position to establish the iteration complexity of \cref{alg:ufgm}, as articulated in the theorem below. 


\begin{theorem}
	\label{thm:ufgm}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then after at most
	\begin{equation*}
		O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \dfrac{1}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}} }
	\end{equation*}
	iterations, \cref{alg:ufgm} will reach an iterate $\bfu_k$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
\end{theorem}


\begin{proof}
	In view of relationship~\cref{eq:stepsize}, the number of line-search steps $j_k$ in \cref{eq:line} satisfies
	\begin{equation*}
		\begin{aligned}
			\dfrac{\mu}{\nu_k^2} \eta_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
			\leq 2 \max_{i \in [m]} \hkh{ \fkh{ \dfrac{2 (1 - \alpha_i)}{\mu (1 + \alpha_i) \varepsilon^2} }^{(1 - \alpha_i) / (1 + \alpha_i)} L_i^{2 / (1 + \alpha_i)} }
			\leq \dfrac{2 M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}},
		\end{aligned}
	\end{equation*}
	where $M > 0$ is a constant defined in \cref{eq:const-m}.
	Since $\eta_k = \nu_k / (1 + \nu_k) \geq \nu_k / 2$, we arrive at
	\begin{equation}
		\label{eq:theta-sigma}
		\dfrac{\nu_k^2}{\mu}
		\geq \dfrac{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}{2 M} \eta_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
		\geq \dfrac{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}}{2^{2 / (1 + \hat{\alpha})} M} \nu_k^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}.
	\end{equation}
	Let $\omega > 0$ be a constant defined as
	\begin{equation*}
		\omega = \dfrac{1}{2^{2 / (1 + 3 \hat{\alpha})}} \fkh{ \dfrac{\mu}{M} }^{(1 + \hat{\alpha}) / (1 + 3 \hat{\alpha})}.
	\end{equation*}
	Then it follows from relationship~\cref{eq:theta-sigma} that
	\begin{equation}
		\label{eq:theta}
		\nu_k \geq \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})},
	\end{equation}
	which further infers that
	\begin{equation*}
		\sigma_{k + 1}
		= (1 + \nu_k) \sigma_k
		\geq \dkh{ 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} } \sigma_k.
	\end{equation*}
	Applying the above inequality for $k$ times recursively yields that
	\begin{equation*}
		\sigma_k \geq \dkh{ 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} }^k.
	\end{equation*}
	As a direct consequence of \cref{eq:dist} and \cref{eq:error}, we can show that
	\begin{equation*}
		\begin{aligned}
			\norm{\bfu_k - \bfu\uast}^2
			\leq {} & \dfrac{2}{\mu} \dkh{f (\bfu_k) - f\uast}
			\leq \dfrac{2}{\mu} \dkh{ \dfrac{1}{\sigma_k} \phi_0 (\bfu\uast) + \dfrac{\mu \varepsilon^2}{4} } \\
			\leq {} & \chi \dkh{ 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} }^{- k}
			+ \dfrac{\varepsilon^2}{2},
		\end{aligned}
	\end{equation*}
	where $\chi = 2 (f (\bfu_0) - f\uast) / \mu + \norm{\bfu_0 - \bfu\uast}^2 > 0$ is a constant.
	Let $K\uast_\varepsilon$ be the smallest iteration number $k$ such that $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$.
	By solving the inequality $\chi ( 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} )^{- k} \leq \varepsilon^2 / 2$, we have
	\begin{equation*}
		K\uast_\varepsilon
		\leq \log \dkh{ \dfrac{\sqrt{2 \chi}}{\varepsilon} } \dfrac{2}{\log \dkh{ 1 + \omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})} }}
		\leq \log \dkh{ \dfrac{\sqrt{2 \chi}}{\varepsilon} } \dfrac{4}{\omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}}.
	\end{equation*}
	The proof is completed.
\end{proof}


The complexity bound established in \cref{thm:ufgm} is markedly lower than those presented in \cref{thm:gd,thm:upgm}, thereby highlighting the acceleration effect attained by \cref{alg:ufgm}.
Finally, we demonstrate that the number of line-search steps required by \cref{alg:ufgm} is also $O ( \log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + 3 \hat{\alpha})} )$.


\begin{corollary}
	Let $\varepsilon \in (0, 1)$ be a sufficiently small constant.
	Then, to achieve an iterate $\bfu_k$ satisfying $\norm{\bfu_k - \bfu\uast} \leq \varepsilon$, \cref{alg:ufgm} requires at most
	\begin{equation*}
		O \dkh{ \log \dkh{ \dfrac{1}{\varepsilon} } \dfrac{1}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}} }
	\end{equation*}
	line-search steps.
\end{corollary}


\begin{proof}
	It follows from relationship~\cref{eq:theta-sigma} that
	\begin{equation*}
		\rho_{k + 1}
		= 2^{j_k} \rho_k
		= \dfrac{\mu}{\nu_k^2}
		\leq \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \fkh{ \dfrac{1}{\nu_k} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})},
	\end{equation*}
	which together with \cref{eq:theta} implies that
	\begin{equation*}
		\rho_{k + 1}
		\leq \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\varepsilon^{2 (1 - \hat{\alpha}) / (1 + \hat{\alpha})}} \fkh{ \dfrac{1}{\omega \varepsilon^{2 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}} }^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})}
		= \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\omega^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \varepsilon^{4 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}}.
	\end{equation*}
	Let $N_k$ be the total number of line-search steps after $k$ iterations in \cref{alg:ufgm}.
	In view of \cref{eq:nk}, we have
	\begin{equation*}
		\begin{aligned}
			N_k
			\leq{} & k + 1 + \log \dkh{ \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\omega^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \varepsilon^{4 (1 - \hat{\alpha}) / (1 + 3 \hat{\alpha})}} } - \log \rho_0 \\
			\leq {} & k
			+ \dfrac{4 (1 - \hat{\alpha})}{1 + 3 \hat{\alpha}} \log \dkh{\dfrac{1}{\varepsilon}}
			+ \log \dkh{ \dfrac{2^{2 / (1 + \hat{\alpha})} M}{\omega^{(1 - \hat{\alpha}) / (1 + \hat{\alpha})} \rho_0} }
			+ 1.
		\end{aligned}
	\end{equation*}
	Consequently, \cref{thm:ufgm} indicates that the total number of line-search steps in \cref{alg:ufgm} is at most $O ( \log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + 3 \hat{\alpha})} )$, which completes the proof.
\end{proof}


\begin{remark}
	By an analogous argument, we can also prove that \cref{alg:ufgm} requires at most $O ( \log (\varepsilon^{-1}) \varepsilon^{(\hat{\alpha} - 1) / (1 + 3 \hat{\alpha})} )$ iterations to generate an iterate $\bfu_k$ such that $f (\bfu_k) - f\uast \leq \varepsilon$ for problem~\cref{opt:main}. 
	Very recently, Doikov \cite{Doikov2025lower} has shown that, in the case $m = 2$, where $f_1$ is a convex function with a H{\"o}lder continuous gradient and $f_2 (\bfu) = \norm{\bfu}^2$, the lower complexity bound for first-order methods is precisely $O ( \log (\varepsilon^{-1}) \varepsilon^{(\hat{\alpha} - 1) / (1 + 3 \hat{\alpha})} )$ in terms of function value accuracy. 
	This finding confirms that \cref{alg:ufgm} achieves the optimal iteration complexity. 
\end{remark}



\section{Numerical Experiments}

\label{sec:numerical}


Preliminary numerical results are presented in this section to provide additional insights into the performance guarantees of the proposed algorithms. 
We aim to elucidate that the final error is influenced by both the stepsize and the H{\"o}lder exponent. 
%All codes are implemented in MATLAB R2018b on a workstation with dual Intel Xeon Gold 6242R CPU processors (at $3.10$ GHz$\times 20 \times 2$) and $510$ GB of RAM under Ubuntu 20.04.
The numerical results are generated by Julia \cite{Bezanson2017julia} (version 1.12) on an Apple Macintosh Mini with an M2 processor, 8 performance cores, and 32GB of memory. 
We have placed the Julia codes in the GitHub repository \url{https://github.com/ctkelley/Grad_Des_CKW.jl} with instructions for reproducing the figures. 

\subsection{Two-dimensional PDE with a non-Lipschitz term}

\label{subsec:example}

H{\"o}lder continuous gradients arise naturally in partial differential equations (PDEs) involving non-Lipschitz nonlinearity \cite{Barrett1991finite,Tang2025uniqueness}. 
In this subsection,  we introduce a numerical example from \cite{Barrett1991finite}. 
This problem is to solve the following two-dimensional PDE,
\begin{equation}
	\label{eq:cF}
	\cF (u) = - \Delta u + \nu u_+^{p} = 0,
\end{equation}
where $p \in (0,1)$, $\nu > 0$ is a constant and $u_+ = \max \{u, 0\}$.
It should be noted that $\cF$ is the gradient of the following energy functional,
\begin{equation*}
	\hat{f} (u) = \frac{1}{2} \|\nabla u\|^2 
	+ \frac{\nu}{p+1} \int_D u_+^{p+1} (y) \, \rmd y.
\end{equation*}
%defined for $u \in H^1(D)$.


Discretizing \cref{eq:cF} with the standard five point difference scheme \cite{LeVeque2007finite} leads to the following nonlinear system,
\begin{equation}
	\label{F0}
	\bfF (\bfu) = \bfA \bfu + \nu \bfu_+^{1/2} - \bfb = 0,
\end{equation}
where $\bfA \in \Rnn$ is the discretization of $- \Delta$ with zero boundary conditions, $\bfb \in \Rn$ encodes the boundary conditions, and $\bfu_+^{1/2} = \max \{\bfu, 0\}^{1/2}$ is understood as a component-wise operation. 
Problem~\cref{F0} is equivalent to optimization problem~\cref{opt:main} with $\Omega=\mathbb{R}^n$, and
\begin{equation*}
	f (\bfu) =  \dfrac{1}{2}(f_1 (\bfu)+ f_2 (\bfu)) \quad
	{\rm with} \quad  f_1 (\bfu)= \bfu\zz \bfA \bfu - 2\bfb\zz \bfu,
	\quad  f_2 (\bfu)=\frac{\nu}{p+1} \bfe\zz \bfu_+^{1+p},
\end{equation*}
where $\bfe \in \Rn$ is the vector of all ones. 


It is clear that $\nabla f_1$ is Lipschitz continuous with the Lipschitz constant  $L_1 = \norm{\bfA}$, and $\nabla f_2$ is locally H{\"o}lder continuous 
with $\alpha = 1/2$ and $L_2 = \nu n^{1/4}$ from
\begin{equation*}
	\norm{\nabla f_2 (\bfu) - \nabla f_2 (\bfv) }
	= \nu \norm{\bfu_+^{1/2} - \bfv_+^{1/2}}
	\leq \nu n^{1/4} \norm{\bfu - \bfv}^{1/2},
\end{equation*}
for all $\bfu, \bfv \in \Rn$. The function $f$ is $\lambda(\bfA)$-strongly convex, where
$\lambda(\bfA)$ is the smallest eigenvalue of the symmetric positive definite matrix $\bfA$.


We now modify the problem to enable direct computation of the errors in the iteration. 
To this end we follow Example 4.4 in \cite{Qu2025extra} and take as the exact solution the function
\[
%#u^*(x,y) = \left(\frac{3 r - 1}{2} \right)^{2p/(1-p)} \max(0, r-1/3)
u^*(x,y) = \left(\frac{3 r - 1}{2} \right)^{2} \max(0, r-1/3)
\]
where $r = \sqrt{x^2 + y^2}$, and let $\bfu^*$ be $u^*$ evaluated
at the interior grid points. We enforce the boundary conditions
\[
u(x,1) = u^*(x,1), u(x,0) = u^*(x,0), u(1,y) = u^*(1,y), u(0,y) = u^*(0,y)
\]
for $0 < x,y < 1$ and encode this into $\bfb$
Letting $\bfc^* = \bfF(\bfu*)$ out modified equation is 
\begin{equation}
	\label{eq:problem1}
	\bfF(\bfu) - \bfc^* = 0.
\end{equation}
Equation~\ref{eq:problem1} is the necessary condition for the optimization
problem
\begin{equation} \label{opt:test}
	\min_{\bfu \in \Rn} f (\bfu) = \dfrac{1}{2} \bfu\zz \bfA \bfu + \dfrac{1}{1 + p} \bfe\zz \bfu_{+}^{1 + p} - (\bfc^*)\zz \bfu.
\end{equation}


In the iteration we use the solution of $\bfA \bfu_0 = - \bfb$ as the 
initial iterate. This is the discretization of Laplace's equation
with the problem boundary conditions. In this way we ensure that the
entire iteration satisifies the boundary conditions. We use a $n \times n$
grid with $n=15$ for the examples in this section. 

We then examine the effects of grid refinement in \cref{subsubsec:alg1ex1}.

\subsection{Algorithm 1}
\label{subsubsec:alg1ex1}
In the first experiment, we scrutinize the performance of the gradient descent method \cref{eq:gd} under different stepsizes.
Specifically, with the parameters $p$ and $\nu$ fixed at $0.5$. 

We test the algorithm is tested for stepsizes of the form $\tau = \tau_0 h^2$,
where $h = 1/(n+1)$ is the spatial meshwidth and $\tau_0$ is taken from
the set $\{.2, .1, .05, .01\}$.

The corresponding numerical results, presented in \cref{subfig:stepsize}, illustrate the decay of the distance between the iterates and the global minimizer over iterations.
It can be observed that a larger stepsize facilitates a more rapid descent  in the early stage of iterations, albeit at the expense of a greater asymptotic error.
This phenomenon corroborates our theoretical predictions.


In the second experiment, we fix $\tau_0$ is fixed at $0.01$, 
while the parameter $p$ is varied over the values $\{0.2, 0.4, 0.6, 0.8\}$.
\Cref{subfig:alpha} similarly tracks the decay of the distance to the global minimizer over iterations.
It is evident that, as the value of $p$ decreases, the final error attained by the algorithm increases under the same stepsize.
Therefore, the associated optimization problems become increasingly ill-conditioned and thus more challenging to solve for smaller values of $p$.
These findings offer empirical support for our theoretical analysis.

\begin{figure}[h!]
	\centering
	\subfigure[different stepsizes]{
		\label{subfig:stepsize}
		\includegraphics[width=0.45\linewidth]{Figures/test_stepsize.pdf}
	}
	\subfigure[different values of $p$]{
		\label{subfig:alpha}
		\includegraphics[width=0.45\linewidth]{Figures/test_alpha.pdf}
	}
	\caption{Numerical performance of Algorithm 1 for problem~\cref{opt:test}.}
	\label{fig:gd}
\end{figure}

We now repeat the experiment with $n=31$, so we reduce the mesh width
by a factor of 2 and increase the norm of $\bfA$ by a factor of four.
As one would expect the stepsize must decrease by a factor of four
for stability.

\begin{figure}[h!]
	\centering
	\subfigure[different stepsizes]{
		\label{subfig:stepsize2}
		\includegraphics[width=0.45\linewidth]{Figures/test2_stepsize.pdf}
	}
	\subfigure[different values of $p$]{
		\label{subfig:alpha2}
		\includegraphics[width=0.45\linewidth]{Figures/test2_alpha.pdf}
	}
	\caption{Numerical performance of Algorithm 1 for problem~\cref{opt:test}.}
	\label{fig:gd2}
\end{figure} 


\subsection{Algorithm 2}
\label{subsubsec:alg2ex1}




\subsection{Example 2}
We consider a numerical example motivated by a semi-linear elliptic problem with a constraint on the solution in a certain set \cite{Tang2025uniqueness}.
Let $D=(0,1)^3$ and
\begin{equation} \label{eq:cF2}
	\cH (u) = - \Delta u + \lambda |u|^{\nu} - |u|^{p-1} u
\end{equation}
on  $D$ with the boundary condition $u=1$ on the boundary $\partial D$, where $p > 1$, $\nu \in (0, 1)$ and $\lambda > p/\nu$ are  constants.
We consider the variational inequality that is to find $u^*\in [-1,1]$ such that
for any $u\in [-1,1]$,
\begin{equation*}
	\cH(u^*)(u-u^*)\ge 0.
\end{equation*}
This problem is equivalent to the nonlinear equation
\begin{equation}\label{exampleVI}
	0=\cF(u):=\left\{\begin{array}{ll}
		\cH(u)  & \quad {\rm if} \quad u-\cH(u) \in [-1, 1],\\
		u-1     & \quad {\rm if}  \quad u-\cH(u) \ge 1,\\
		u+1     & \quad {\rm otherwise.}
	\end{array}\right.
\end{equation}
Discretizing \cref{eq:cF2} with the standard five point difference scheme \cite{LeVeque2007finite}, problem~\cref{exampleVI} leads to the following  system of nonlinear equations
\begin{equation}\label{example2}
	\bfF(\bfu) = \bfu-{\mathrm \Pi}_{\bfU}\Big(\bfu- \tau(\bfA \bfu + \lambda |\bfu|^{\nu} - |\bfu|^{p -1}\bfu - \bfb)\Big) = 0,
\end{equation}
where $\bfU=[-1,1]^n$,  $\tau>0$ is a constant, $ \bfA\in \mathbb{R}^{n\times n}$ is a symmetric positive definite matrix and $\bfb\in \mathbb{R}^n.$  Note that \cref{example2} is the first-order optimal condition of the minimization problem
\begin{equation}\label{example2min}
	\min_{\bfu \in [-1,1]^n} f(\bfu):= \frac{1}{2}\bfu^\top\bfA\bfu + \frac{\lambda}{1+\nu} \bfe\zz |\bfu|^{\nu + 1}- \frac{1}{1+p}\bfe\zz  \max(\bfu, -\bfu)^{p+1} + \bfb^\top \bfu.
\end{equation}
The Hessian matrix of $f$ at $\bfu$ with $\bfu_i\neq 0$, $i=1,\ldots,n$ has the form
$$\nabla^2 f(\bfu)=\bfA  + \lambda \nu |\bfu|^{\nu-1} -p {\rm diag} \Big(\max (-\bfu, \bfu)^{p-1}\Big),$$
Since $\lambda\nu>p$, $\nabla^2 f(\bfu)$ is
symmetric positive definite for any $\bfu\in [-1, 1]^n$ with $\bfu_i\neq 0$, $i=1,\ldots,n$. Hence $f$ is $\mu$-strongly convex in $[-1,1]^n$ with $\mu=\lambda_{\min}(\bfA)$  and the system \cref{example2} has a unique solution in
$[-1, 1]^n.$ However, $\nabla f$ is not  Lipschitz continuous in $[-1,1]^n.$

Let
$$f_1(\bfu)=\frac{1}{2}\bfu^\top\bfA\bfu + \bfb^\top \bfu,  f_2(\bfu)=\frac{\lambda}{1+\nu} \bfe\zz |\bfu|^{\nu + 1}, f_3(\bfu)=- \frac{1}{1+p}\bfe\zz  \max(\bfu, -\bfu)^{p+1}$$
This example satisfies Assumption~\ref{asp:function} (ii) with $L_1=\lambda_{\max}(\bfA)$, $L_2=\lambda\nu$, $L_3=pn^{\frac{1}{2}}, \alpha_1=\alpha_3=1, \alpha_2={1-\nu}$.





\section{Conclusion}

\label{sec:conclusion}

In this paper, we consider a class of strongly convex constrained optimization problems of the form \cref{opt:main}.
Example 1.1 shows that although each component function $f_i$ of the ojective function $f$ admits a H{\"o}lder continuous gradient with an component $\alpha_i \in (0, 1]$, the gradient of  $f$ is not necessarily H{\"o}lder continuous.
To establish the iteration complexity of the projected gradient descent methods for this class of problems, we use the parameter $\hat{\alpha} = \min_{i \in [m]} \alpha_i$ to determine the complexity bound.  Algorithm 1 is a new version of projected gradient method for problem \cref{opt:main} with an appropriately fixed stepsize. Theorem 2.2 shows that Algorithm 1
can find an iterate in the feasible set $\Omega$ with a distance to the global minimizer  less than $\varepsilon$ at most $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + \hat{\alpha})})$ iterations.
This recovers the classical complexity result when $\hat{\alpha} = 1$ and reveals the additional difficulty imposed by the weaker smoothness of the objective function for $\hat{\alpha} < 1$. Algorithm 2 is a modification of Algorithm 1 for problems where the parameters $\alpha_i$ and $L_i$ are difficult to estimate for the stepsize. In Algorithm 3, the stepsize is updated by the universal scheme at each iteration, which improves the complexity bound to $O (\log (\varepsilon^{-1}) \varepsilon^{2 (\hat{\alpha} - 1) / (1 + 3 \hat{\alpha})})$.
Numerical experiments are conducted to validate our theoretical findings, demonstrating the expected behavior of projected gradient descent methods under different stepsizes and H{\"o}lder exponents.
These results offer new insights into the performance guarantees of the classic projected gradient descent methods for a broader class of optimization problems with non-Lipschitz gradients.








%\section*{Acknowledgments}
%We would like to acknowledge the assistance of volunteers in putting
%together this example manuscript and supplement.



\bibliographystyle{siamplain}
\bibliography{library_HGD}


\end{document}
